[["another-example-with-hierarchical-model-approach---oven-in-katahdin-woods-and-waters.html", "Section 7 Another example with hierarchical model approach - OVEN in Katahdin Woods and Waters 7.1 Packages 7.2 Loading and exploring data 7.3 Hierarchical model for counts with two observation processes", " Section 7 Another example with hierarchical model approach - OVEN in Katahdin Woods and Waters 7.1 Packages library(tidyverse) library(MetBrewer) library(dclone) library(mcmcplots) 7.2 Loading and exploring data Let’s load the file used in 01_data-comparability.Rmd script. acous_count &lt;- read_csv(&quot;results/pooled_pointCount_acoustic_data.csv&quot;) Let’s work on the species-site combination of more data: Ovenbird in Marsh-Billings-Rockefeller NHP. acous_count_filt &lt;- acous_count |&gt; filter(common_name %in% c( #&quot;American Crow&quot;,&quot;American Robin&quot;,&quot;Black-and-white Warbler&quot;,&quot;Black-capped Chickadee&quot;,&quot;Black-throated Blue Warbler&quot;,&quot;Black-throated Green Warbler&quot;,&quot;Blackburnian Warbler&quot;,&quot;Blue Jay&quot;,&quot;Blue-headed Vireo&quot;,&quot;Brown Creeper&quot;,&quot;Eastern Wood-Pewee&quot;,&quot;Golden-crowned Kinglet&quot;,&quot;Hermit Thrush&quot;,&quot;Northern Parula&quot;, &quot;Ovenbird&quot; #,&quot;Pine Warbler&quot;,&quot;Red-breasted Nuthatch&quot;,&quot;Red-eyed Vireo&quot;,&quot;Scarlet Tanager&quot;,&quot;Winter Wren&quot;,&quot;Yellow-bellied Sapsucker&quot;,&quot;Yellow-rumped Warbler&quot; ), site_name == &quot;Katahdin Woods and Waters&quot;) table(acous_count_filt$common_name, acous_count_filt$data_type) How to reconcile time of detection of each record? While the point_count data include the observation_time variable, this information for the acoustic_data seems to be at the begin_clock_time variable. Let’s adjust this with the function coalesce from dplyr in tidyverse. Then, we can consolidate the “initial temporal window” per visit in each site_id with the detection time. acoust_count_summary &lt;- acous_count_filt |&gt; # dummy column to unify time of detection either by count or mutate(detection_time = coalesce(observation_time, begin_clock_time)) |&gt; # initial temporal window group_by(site_id, date, visit_number) |&gt; mutate(time_window = min(hms::as_hms(detection_time), na.rm = TRUE), time_window = hms::as_hms(time_window)) |&gt; # First summarization to extract sum of counts per point count and number of detections group_by(site_name, site_id, date, visit_number, common_name, data_type, time_window, detection_time) |&gt; summarise(abundance = sum(abundance, na.rm = TRUE), acous_detections = n()) |&gt; # Second summarization to count number of detections and extract overall counts group_by(site_name, site_id, date, visit_number, common_name, time_window) |&gt; summarise(counts = sum(abundance, na.rm = TRUE), acous_detections = sum(acous_detections)) |&gt; as.data.frame() acoust_count_summary$DateTime &lt;- as.POSIXct(paste(acoust_count_summary$date, acoust_count_summary$time_window), format = &quot;%Y-%m-%d %H:%M:%S&quot;) acoust_count_summary$Time_window &lt;- floor_date(acoust_count_summary$DateTime, &quot;10 mins&quot;) str(acoust_count_summary) And we can generate the figures. This are similar to my previous attempt, but including some days or point counts discarded by Vijay due to species richness. 7.3 Hierarchical model for counts with two observation processes 7.3.1 Coding Observation point counts Preparing data for data cloning acoust_count_df &lt;- acoust_count_summary |&gt; mutate(Time_w_alone = format(as.POSIXct(Time_window, format = &quot;%Y-%m-%d %H:%M:%S&quot;), &quot;%H:%M&quot;), counts = counts+1, acous_detections = acous_detections+1, year = year(date), ) |&gt; filter(year == 2023) |&gt; arrange(Time_window) # generate sequence of potential point counts seq_df &lt;- acoust_count_df |&gt; distinct(date) |&gt; # keep only unique sampled dates mutate(seq = map(date, ~ seq( from = as.POSIXct(paste(.x, &quot;05:00:00&quot;)), to = as.POSIXct(paste(.x, &quot;10:00:00&quot;)), by = &quot;10 min&quot; ))) |&gt; unnest(seq) |&gt; mutate(Time_window = seq, Time_w_alone = format(seq, &quot;%H:%M&quot;)) final_df &lt;- seq_df |&gt; select(c(date, Time_w_alone)) |&gt; left_join(acoust_count_df, by = c(&quot;date&quot;, &quot;Time_w_alone&quot;)) final_df$DateTime &lt;- as.POSIXct(paste(final_df$date, final_df$Time_w_alone), format = &quot;%Y-%m-%d %H:%M&quot;) final_df$time_id &lt;- as.integer(factor(final_df$DateTime)) StochGSS.dc &lt;- function(){ # Priors on model parameters. Priors are DC1 in Lele et al (2007) a1 ~ dnorm(0,1); # constant, equivalent to the population growth rate. c1 ~ dunif(-1,1); # constant, the density dependence parameter. sig1 ~ dlnorm(-0.5,10); #variance parameter of stochastic environment (process noise) in the system stovar1 &lt;- 1/pow(sig1,2) tau1 ~ dunif(0,1); # detection probability in Binomial distribution for(k in 1:K){ # Simulate trajectory that depends on the previous mean_X1[1,k] &lt;- a1/(1-c1) # Expected value of the first realization of the process # this is drawn from the stationary distribution of the process # Equation 14 (main text) and A.4 in Appendix of Dennis et al 2006 Varno1[k] &lt;- pow(sig1,2)/(1-pow(c1,2)) #. Equation A.5 in Appendix of Dennis et al 2006 # Updating the state: Stochastic process for all time steps X1[1,k]~dnorm(mean_X1[1,k], 1/Varno1[k]); #first estimation of population #iteration of the GSS model in the data for (t in 2:qp1) { mean_X1[t,k] &lt;- a1 + c1 * X1[(t - 1),k] X1[t,k] ~ dnorm(mean_X1[t,k], stovar1) # Et is included here since a+cX(t-1) + Et ~ Normal(a+cX(t-1),sigma^2) } # Updating the observations, from the counts under Binomial observation error for (t in 1:qp1) { N1[t,k] ~ dpois(exp(X1[t,k])) # trick to convert continuous exp(X1) to integer counts as trials Y1[t,k] ~ dbin(tau1, N1[t,k]) # } } } For this model, we will need some initial values of the parameters, and given that there could be many NA a wrap to provide those initial guess values. guess.calc &lt;- function(Yobs,Tvec){ T.t &lt;-Tvec-Tvec[1]; # For calculations, time starts at zero. q &lt;- length(Yobs)-1; # Number of time series transitions, q. qp1 &lt;- q+1; # q+1 gets used a lot, too. S.t &lt;- T.t[2:qp1]-T.t[1:q]; # Time intervals. Ybar &lt;- mean(Yobs); Yvar &lt;- sum((Yobs-Ybar)*(Yobs-Ybar))/q; mu1 &lt;- Ybar; # Kludge an initial value for theta based on mean of Y(t+s) given Y(t). th1&lt;- -mean(log(abs((Yobs[2:qp1]-mu1)/(Yobs[1:q]-mu1)))/S.t); bsq1&lt;- 2*th1*Yvar/(1+2*th1); # Moment estimate using stationary tsq1&lt;- bsq1; # variance, with betasq=tausq. #three 0&#39;s three0s &lt;- sum(c(th1,bsq1,tsq1)) if(three0s==0|is.na(three0s)){th1 &lt;- 0.5;bsq1 &lt;- 0.09; tsq1 &lt;- 0.23;} out1 &lt;- c(th1,bsq1,tsq1); if(sum(out1&lt;1e-7)&gt;=1){out1 &lt;- c(0.5,0.09,0.23)} out &lt;- c(mu1,out1); return(abs(out)) } guess.calc2.0&lt;- function(TimeAndNs){ newmat &lt;- TimeAndNs isnas &lt;- sum(is.na(TimeAndNs)) if(isnas &gt;= 1){ isnaind &lt;- which(is.na(TimeAndNs[,2]), arr.ind=TRUE) newmat &lt;- TimeAndNs[-isnaind,] newmat[,1] &lt;- newmat[,1] - newmat[1,1] } init.guess &lt;- guess.calc(Yobs = log(newmat[,2]), Tvec=newmat[,1]) mu1 &lt;- init.guess[1] th1 &lt;- init.guess[2] bsq1 &lt;- init.guess[3] sigsq1&lt;- ((1-exp(-2*th1))*bsq1)/(2*th1) out &lt;- c(mu=mu1, theta=th1, sigmasq = sigsq1) return(out) } And we have to bundle the data for Data cloning. Let’s fit the first species in the taxa with higher data (Ovenbird in Marsh-Billings-Rockefeller NHP) as a test. ts.4guess &lt;- final_df$counts tvec4guess &lt;- 1:length(ts.4guess) onets4guess &lt;- cbind(tvec4guess, ts.4guess) naive.guess &lt;- guess.calc2.0(TimeAndNs = onets4guess) datalistGSS.dc &lt;- list(K = 1, qp1 = length(ts.4guess), Y1 = dcdim(array(ts.4guess, dim = c(length(ts.4guess),1)))) dcrun.GSS &lt;- dc.fit(data = datalistGSS.dc, params = c(&quot;a1&quot;, &quot;c1&quot;, &quot;sig1&quot;, &quot;tau1&quot;), model = StochGSS.dc, n.clones = c(1,5,10,20), multiply = &quot;K&quot;, unchanged = &quot;qp1&quot;, n.chains = 3, n.adapt = 5000, n.update = 100, thin = 20, n.iter = 20000) saveRDS(dcrun.GSS, &quot;data/dcfitGSS_2023_OVEN_KWW.rds&quot;) dcrun.GSS &lt;- readRDS(&quot;data/dcfitGSS_2023_OVEN_KWW.rds&quot;) summary(dcrun.GSS); dcdiag(dcrun.GSS) plot(dcdiag(dcrun.GSS)) pairs(dcrun.GSS) coef(dcrun.GSS) Data cloning results for four parameters of GSS from point counts And with these coefficients of the model, we can estimate latent count trajectories with the Kalman filter structure. This Kalman filter structure allows simultaneous estimation of latent states for observed time steps and prediction for missing ones, leveraging the temporal correlation in the process model. Kalman.pred.fn &lt;- function() { # Priors on model parameters: they are on the real line. parms ~ dmnorm(MuPost,PrecPost) a1 &lt;- parms[1] c1 &lt;- parms[2] sig1 &lt;- parms[3] stovar1 &lt;- 1/pow(sig1,2) tau1 ~ dunif(0,1) # Likelihood mean_X1[1] &lt;- a1/(1-c1) # Expected value of the first realization of the process # this is drawn from the stationary distribution of the process # Equation 14 (main text) and A.4 in Appendix of Dennis et al 2006 Varno1 &lt;- pow(sig1,2)/(1-pow(c1,2)) #. Equation A.5 in Appendix of Dennis et al 2006 # Updating the state: Stochastic process for all time steps X1[1]~dnorm(mean_X1[1], 1/Varno1); #first estimation of population C[1] &lt;- exp(X1[1]) #iteration of the GSS model in the data for (t in 2:qp1) { mean_X1[t] &lt;- a1 + c1 * X1[(t - 1)] X1[t] ~ dnorm(mean_X1[t], stovar1) # Et is included here since a+cX(t-1) + Et ~ Normal(a+cX(t-1),sigma^2) N[t] ~ dpois(exp(X1[t])) # trick to convert continuous exp(X1) to integer counts as trials in binomial Y1[(t-1)] ~ dbin(tau1, N[t]) # C[t] &lt;- exp(X1[t]) # expected count } } data4kalman &lt;- list(qp1 = as.numeric(length(final_df$counts)), Y1 = array(final_df$counts, dim = c(as.numeric(length(final_df$counts)))), MuPost = coef(dcrun.GSS), PrecPost = solve(vcov(dcrun.GSS))) And run the Bayesian inference using the MLE from Data cloning BH_DC_Pred = jags.fit(data=data4kalman, params=c(&quot;C&quot;), model=Kalman.pred.fn) And we can generate a dataframe of the time series with the estimates, inter quartile range, and observed data through time summary(BH_DC_Pred) # extract predictions and CI around them pred &lt;- as.data.frame(t(mcmcapply(BH_DC_Pred, quantile, c(0.025, 0.5, 0.975)))) ExpectedCounts &lt;- as.data.frame(cbind(final_df,pred)) # modify names names(ExpectedCounts) &lt;- c(&quot;date&quot;,&quot;Time_w_alone&quot;,&quot;site_name&quot;,&quot;site_id&quot;, &quot;visit_number&quot;,&quot;common_name&quot;,&quot;time_window&quot;, &quot;Observed&quot;, &quot;acous_detections&quot;,&quot;DateTime&quot;,&quot;Time_window&quot;, &quot;year&quot;, &quot;time_id&quot;,&quot;Lower&quot;, &quot;Estimated&quot;, &quot;Upper&quot;) ExpectedCounts |&gt; pivot_longer(cols = c(Lower, Estimated, Upper, Observed), names_to = &quot;Abundance&quot;, values_to = &quot;Count&quot;) |&gt; ggplot(aes(x = DateTime, y = Count, color = factor(Abundance, levels = c(&quot;Observed&quot;, &quot;Upper&quot;, &quot;Estimated&quot;, &quot;Lower&quot;)))) + geom_line(aes(linetype = factor(Abundance, levels = c(&quot;Observed&quot;, &quot;Upper&quot;, &quot;Estimated&quot;, &quot;Lower&quot;)))) + geom_point(aes(shape = factor(Abundance, levels = c(&quot;Observed&quot;, &quot;Upper&quot;, &quot;Estimated&quot;, &quot;Lower&quot;)))) + scale_y_continuous(limits = c(0,10))+ labs(title = &quot;OVEN - Katahdin Woods and Waters - 2023 sampling&quot;, x = &quot;Time (sampling window)&quot;, y = &quot;Counts&quot;, color = &quot;Counts&quot;, linetype = &quot;Counts&quot;, shape = &quot;Counts&quot;) + scale_linetype_manual(values = c(NA,&quot;dashed&quot;,&quot;solid&quot;,&quot;dashed&quot;))+ scale_shape_manual(values = c(21, NA,NA,NA)) + scale_color_manual(values = c(&quot;blue&quot;,&quot;darkgray&quot;,&quot;black&quot;,&quot;darkgray&quot;)) + theme_classic() + theme(legend.position = &quot;bottom&quot;) 7.3.2 Coding Acoustic detections counts StochGSS.acou.dc &lt;- function(){ # Priors on model parameters. Priors are DC1 in Lele et al (2007) a1 ~ dnorm(0,1); # constant, equivalent to the population growth rate. c1 ~ dunif(-1,1); # constant, the density dependence parameter. sig1 ~ dlnorm(-0.5,10); #variance parameter of stochastic environment (process noise) in the system stovar1 &lt;- 1/pow(sig1,2) # Priors for observation model alpha ~ dnorm(0, 0.001) # intercept for scaling gamma ~ dnorm(0, 0.001) # slope linking latent state X_t to mean r ~ dunif(0,50) # dispersion parameter for NegBin for(k in 1:K){ # Simulate trajectory that depends on the previous mean_X1[1,k] &lt;- a1/(1-c1) # Expected value of the first realization of the process # this is drawn from the stationary distribution of the process # Equation 14 (main text) and A.4 in Appendix of Dennis et al 2006 Varno1[k] &lt;- pow(sig1,2)/(1-pow(c1,2)) #. Equation A.5 in Appendix of Dennis et al 2006 # Updating the state: Stochastic process for all time steps X1[1,k]~dnorm(mean_X1[1,k], 1/Varno1[k]); #first estimation of population #iteration of the GSS model in the data for (t in 2:qp1) { mean_X1[t,k] &lt;- a1 + c1 * X1[(t - 1),k] X1[t,k] ~ dnorm(mean_X1[t,k], stovar1) # Et is included here since a+cX(t-1) + Et ~ Normal(a+cX(t-1),sigma^2) } # Updating the observations, from the acoustic detections under Negative Binomial observation error for (t in 1:qp1) { Y1[t,k] ~ dnegbin(p[t,k], r) p[t,k] &lt;- r/(r+lambda[t,k]) log(lambda[t,k]) &lt;- alpha + gamma * X1[t,k] } } } For this model, we will need some initial values of the parameters, and given that there could be many NA a wrap to provide those initial guess values. And we have to bundle the data for Data cloning. Let’s fit the first species in the taxa with higher data (Ovenbird in Marsh-Billings-Rockefeller NHP) as a test. ts.4guess &lt;- as.numeric(final_df$acous_detections) tvec4guess &lt;- 1:length(ts.4guess) onets4guess &lt;- cbind(tvec4guess, ts.4guess) naive.guess &lt;- guess.calc2.0(TimeAndNs = onets4guess) datalistGSS.acou.dc &lt;- list(K = 1, qp1 = length(ts.4guess), Y1 = dcdim(array(ts.4guess, dim = c(length(ts.4guess),1)))) dcrun.GSS.acou &lt;- dc.fit(data = datalistGSS.acou.dc, params = c(&quot;a1&quot;, &quot;c1&quot;, &quot;sig1&quot;, &quot;gamma&quot;, &quot;alpha&quot;, &quot;r&quot;), model = StochGSS.acou.dc, n.clones = c(1,5,10,20), multiply = &quot;K&quot;, unchanged = &quot;qp1&quot;, n.chains = 3, n.adapt = 5000, n.update = 100, thin = 20, n.iter = 20000) saveRDS(dcrun.GSS.acou, &quot;data/dcfitGSS_AcDet_2023_OVEN_KWW.rds&quot;) dcrun.GSS.acou &lt;- readRDS(&quot;data/dcfitGSS_AcDet_2023_OVEN_KWW.rds&quot;) summary(dcrun.GSS.acou); dcdiag(dcrun.GSS.acou) plot(dcdiag(dcrun.GSS.acou)) pairs(dcrun.GSS.acou) coef(dcrun.GSS.acou) Data cloning results for six parameters of GSS from acoustic detections Although diagnostics for lambda.max decreases with more clones, the chains are not converging. It seems that this needs more iterations and change initial values. Anyway, with these coefficients of the model, we can estimate latent count trajectories with the Kalman filter structure. This Kalman filter structure allows simultaneous estimation of latent states for observed time steps and prediction for missing ones, leveraging the temporal correlation in the process model. Kalman.pred.acou.fn &lt;- function() { # Priors on model parameters: they are on the real line. parms ~ dmnorm(MuPost,PrecPost) a1 &lt;- parms[1] alpha &lt;- parms[2] # intercept for scaling c1 &lt;- parms[3] gamma &lt;- parms[4] # slope linking latent state X_t to mean r &lt;- parms[5] # overdispersion parameter for NegBinom sig1 &lt;- parms[6] stovar1 &lt;- 1/pow(sig1,2) # Likelihood mean_X1[1] &lt;- a1/(1-c1) # Expected value of the first realization of the process # this is drawn from the stationary distribution of the process # Equation 14 (main text) and A.4 in Appendix of Dennis et al 2006 Varno1 &lt;- pow(sig1,2)/(1-pow(c1,2)) #. Equation A.5 in Appendix of Dennis et al 2006 # Updating the state: Stochastic process for all time steps X1[1]~dnorm(mean_X1[1], 1/Varno1); #first estimation of population log(lambda[1]) &lt;- alpha + gamma * X1[1] C[1] &lt;- log(lambda[1]) #iteration of the GSS model in the data for (t in 2:qp1) { mean_X1[t] &lt;- a1 + c1 * X1[(t - 1)] X1[t] ~ dnorm(mean_X1[t], stovar1) # Et is included here since a+cX(t-1) + Et ~ Normal(a+cX(t-1),sigma^2) Y1[(t-1)] ~ dnegbin(p[t], r) p[t] &lt;- r/(r+lambda[t]) log(lambda[t]) &lt;- alpha + gamma * X1[t] C[t] &lt;- log(lambda[t]) # expected count } } data4kalmanAco &lt;- list(qp1 = as.numeric(length(final_df$acous_detections)), Y1 = array(final_df$acous_detections, dim = c(as.numeric(length(final_df$acous_detections)))), MuPost = coef(dcrun.GSS.acou), PrecPost = solve(vcov(dcrun.GSS.acou))) And run the Bayesian inference using the MLE from Data cloning BH_DC_Pred_Acou = jags.fit(data=data4kalmanAco, params=c(&quot;C&quot;), model=Kalman.pred.acou.fn) And we can generate a dataframe of the time series with the estimates, inter quartile range, and observed data through time summary(BH_DC_Pred_Acou) # extract predictions and CI around them predAcou &lt;- as.data.frame(t(mcmcapply(BH_DC_Pred_Acou, quantile, c(0.025, 0.5, 0.975)))) ExpectedCountsAcou &lt;- as.data.frame(cbind(ExpectedCounts,predAcou)) # modify names names(ExpectedCountsAcou) &lt;- c(&quot;date&quot;,&quot;Time_w_alone&quot;,&quot;site_name&quot;,&quot;site_id&quot;, &quot;visit_number&quot;,&quot;common_name&quot;,&quot;time_window&quot;, &quot;Observed&quot;, &quot;AcousticDetections&quot;, &quot;DateTime&quot;,&quot;Time_window&quot;, &quot;year&quot;, &quot;time_id&quot;, &quot;CountLower&quot;, &quot;CountEstimated&quot;, &quot;CountUpper&quot;, &quot;AcouDetLower&quot;, &quot;AcouDetEstimated&quot;, &quot;AcouDetUpper&quot;) par(mar = c(5, 4, 7, 2)) # low, left, up, right plot(x = ExpectedCountsAcou$DateTime, y = ExpectedCountsAcou$Observed, col = &quot;#1b7837&quot;, ylim = c(0,40), ylab = &quot;Counts&quot;, xlab = &quot;Time (sampling window)&quot;) points(x = ExpectedCountsAcou$DateTime, y = ExpectedCountsAcou$AcousticDetections, col = &quot;#762a83&quot;) lines(x = ExpectedCountsAcou$DateTime, y = ExpectedCountsAcou$CountEstimated, col = &quot;#7fbf7b&quot;) lines(x = ExpectedCountsAcou$DateTime, y = ExpectedCountsAcou$CountLower, col = &quot;#d9f0d3&quot;, lty = 2) lines(x = ExpectedCountsAcou$DateTime, y = ExpectedCountsAcou$CountUpper, col = &quot;#d9f0d3&quot;, lty = 2) lines(x = ExpectedCountsAcou$DateTime, y = ExpectedCountsAcou$AcouDetEstimated, col = &quot;#af8dc3&quot;) lines(x = ExpectedCountsAcou$DateTime, y = ExpectedCountsAcou$AcouDetLower, col = &quot;#e7d4e8&quot;, lty = 2) lines(x = ExpectedCountsAcou$DateTime, y = ExpectedCountsAcou$AcouDetUpper, col = &quot;#e7d4e8&quot;, lty = 2) legend(x = min(ExpectedCountsAcou$DateTime), y = 65, cex = 0.7, pt.cex = 0.7, xpd = TRUE, horiz = FALSE, legend = c(&quot;Observed from point counts&quot;, &quot;Acoustic detections&quot;, &quot;Estimate from point counts&quot;, &quot;Point count CI estimates&quot;, &quot;Estimate from acoustic detections&quot;, &quot;Acoustic detections CI estimates&quot;), col = c(&quot;#1b7837&quot;, &quot;#762a83&quot;, &quot;#7fbf7b&quot;, &quot;#d9f0d3&quot;, &quot;#af8dc3&quot;, &quot;#e7d4e8&quot;), lty = c(NA, NA, 1, 2, 1, 2), # NA for points, 1 for solid lines, 2 for dashed pch = c(1, 1, NA, NA, NA, NA), # symbols for points bty = &quot;n&quot;) # no box around legend Observed and estimated counts from point counts and acoustic detections - A Cutting the edges to see the details of the estimates: par(mar = c(6, 4, 4, 2)) plot(x = ExpectedCountsAcou$DateTime, y = ExpectedCountsAcou$Observed, col = &quot;#1b7837&quot;, ylim = c(0,10), ylab = &quot;Counts&quot;, xlab = &quot;Time (sampling window)&quot;, main = &quot;OVEN - 2023 - Katahdin Woods and Waters&quot;) points(x = ExpectedCountsAcou$DateTime, y = ExpectedCountsAcou$AcousticDetections, col = &quot;#762a83&quot;) lines(x = ExpectedCountsAcou$DateTime, y = ExpectedCountsAcou$CountEstimated, col = &quot;#7fbf7b&quot;) lines(x = ExpectedCountsAcou$DateTime, y = ExpectedCountsAcou$CountLower, col = &quot;#d9f0d3&quot;, lty = 2) lines(x = ExpectedCountsAcou$DateTime, y = ExpectedCountsAcou$CountUpper, col = &quot;#d9f0d3&quot;, lty = 2) lines(x = ExpectedCountsAcou$DateTime, y = ExpectedCountsAcou$AcouDetEstimated, col = &quot;#af8dc3&quot;) lines(x = ExpectedCountsAcou$DateTime, y = ExpectedCountsAcou$AcouDetLower, col = &quot;#e7d4e8&quot;, lty = 2) lines(x = ExpectedCountsAcou$DateTime, y = ExpectedCountsAcou$AcouDetUpper, col = &quot;#e7d4e8&quot;, lty = 2) legend(x = quantile(ExpectedCountsAcou$DateTime, 0.75), y = 10, cex = 0.7, pt.cex = 0.7, xpd = TRUE, horiz = FALSE, legend = c(&quot;Observed from point counts&quot;, &quot;Acoustic detections&quot;, &quot;Estimate from point counts&quot;, &quot;Point count CI estimates&quot;, &quot;Estimate from acoustic detections&quot;, &quot;Acoustic detections CI estimates&quot;), col = c(&quot;#1b7837&quot;, &quot;#762a83&quot;, &quot;#7fbf7b&quot;, &quot;#d9f0d3&quot;, &quot;#af8dc3&quot;, &quot;#e7d4e8&quot;), lty = c(NA, NA, 1, 2, 1, 2), # NA for points, 1 for solid lines, 2 for dashed pch = c(1, 1, NA, NA, NA, NA), # symbols for points bty = &quot;n&quot;) # no box around legend Observed and estimated counts from point counts and acoustic detections - B "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
