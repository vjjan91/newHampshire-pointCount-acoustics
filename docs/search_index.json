[["index.html", "Source code and supporting information for _Predicting abundances from acoustic data: North Eastern Bird Communities Section 1 Introduction 1.1 Attribution 1.2 Data access 1.3 Data processing", " Source code and supporting information for _Predicting abundances from acoustic data: North Eastern Bird Communities Vijay Ramesh Pooja Panwar Orlando Acevedo-Charry Sharon Martinson Laurel Symes Last compiled on 25 November, 2025 Section 1 Introduction This is the readable version that showcases analyses associated with predicting abundance from acoustic data using a paired dataset from New Hampshire. 1.1 Attribution Please contact the following in case of interest in the project. Vijay Ramesh (repo maintainer) Postdoctoral Research Associate, Cornell Lab of Ornithology 1.2 Data access The data used in this work will be archived on Zenodo. 1.3 Data processing The data processing for this project is described in the following sections. Navigate through them using the links in the sidebar. "],["data-comparability.html", "Section 2 Data comparability 2.1 Load necessary libraries 2.2 Loading point count and acoustic data 2.3 Cleaning up point count data 2.4 Cleaning up acoustic data 2.5 Creating a combined dataset", " Section 2 Data comparability In this script, we process the point-count and acoustic data and create a combined dataset for downstream analysis. 2.1 Load necessary libraries library(tidyverse) library(dplyr) library(stringr) library(vegan) library(scico) library(data.table) library(extrafont) library(sf) library(raster) library(scales) library(ggplot2) library(ggspatial) library(colorspace) library(lubridate) library(naniar) 2.2 Loading point count and acoustic data point_count &lt;- read.csv(&quot;data/point-count-data.csv&quot;) acoustic_data &lt;- read.csv(&quot;data/acoustic-data.csv&quot;) 2.3 Cleaning up point count data # include distance information in lieu of the categories point_count &lt;- point_count %&gt;% mutate(distance = case_when(distance_band == 1 ~ &quot;0-10&quot;, distance_band == 2 ~ &quot;10-25&quot;, distance_band == 3 ~ &quot;25-50&quot;, distance_band == 4 ~&quot;&gt;50&quot;, distance_band == NA ~ NA)) # convert dates to standard YYYYMMDD format point_count$date &lt;- mdy(point_count$date) # convert start time and observation time to lubridate format point_count$start_time &lt;- hms::as_hms(point_count$start_time) point_count$observation_time &lt;- hms::as_hms(point_count$observation_time) # remove incidental observations point_count &lt;- point_count %&gt;% filter(observation_method != &quot;I&quot;) # remove non-bird species and unidentified/NA values point_count &lt;- point_count %&gt;% filter(!common_name %in% c(&quot;Unidentified Woodpecker&quot;, &quot;Eastern Chipmunk&quot;, &quot;Eastern Gray Squirrel&quot;, &quot;Red Squirrel&quot;, NA)) # using the library naniar, we assess missing data/NANs or if exploratory processing of data did not clean up the NAs miss_var_summary(point_count) # above procedure revealed NAs in the observation_time column which are associated with data on Canada Geese from MABI1002 and MABI1001 on 5/22/2023. We will not delete this data # other columns/missing data in those can be ignored as distance was not calculated for all observations due to variation in survey_observers 2.4 Cleaning up acoustic data # ensure begin clock and end clock time are in HHMMSS format # note: alternately, the begin and end clock time could be potentially extracted from the selection tables (if present) # appending a zero to begin and end clock time acoustic_data$begin_clock_time &lt;- paste0(&quot;0&quot;, acoustic_data$begin_clock_time) acoustic_data$ end_clock_time &lt;- paste0(&quot;0&quot;, acoustic_data$end_clock_time) # write a function to convert to HH:MM:SS convert_to_time &lt;- function(x) { # convert string to numeric x &lt;- as.numeric(x) hours &lt;- floor(x/10000) minutes &lt;- floor((x - hours*10000)/100) seconds &lt;- x - hours*10000 - minutes*100 sprintf(&quot;%02d:%02d:%02d&quot;, hours, minutes, seconds) } # make the conversion acoustic_data$begin_clock_time &lt;- sapply(acoustic_data$begin_clock_time, convert_to_time) acoustic_data$end_clock_time&lt;- sapply(acoustic_data$end_clock_time, convert_to_time) # convert to hms format to keep it consistent with point_count data acoustic_data$begin_clock_time&lt;- hms::as_hms(acoustic_data$begin_clock_time) acoustic_data$end_clock_time &lt;- hms::as_hms(acoustic_data$end_clock_time) # make the data column similar to point_count data in YYYY-MM-DD format acoustic_data$date &lt;- ymd(acoustic_data$date) # extract year column acoustic_data$year &lt;- lubridate::year(acoustic_data$date) # remove non-bird species and unidentified/NA values acoustic_data &lt;- acoustic_data %&gt;% filter(!common_name %in% c(&quot;Eastern Chipmunk&quot;, &quot;Red Squirrel&quot;, NA)) # using the library naniar, we assess missing data/NANs or if exploratory processing of data did not clean up the NAs miss_var_summary(acoustic_data) # the column &#39;background&#39; has a large number of NAs, which can be ignored since we will not be using this column in future analysis. 2.5 Creating a combined dataset # include the data_type for each dataset acoustic_data &lt;- acoustic_data %&gt;% mutate(data_type = &quot;acoustic_data&quot;) point_count &lt;- point_count %&gt;% mutate(data_type = &quot;point_count&quot;) # before we combine the two datasets, we want to ensure that both dates and site_id match across the two datasets. In other words, we do not want visits in which in which either acoustic data or point count data was not collected, since this is a simultaneous comparison # find dates to remove by checking for mismatches between the two datasets using both site_id and date dates_to_remove &lt;- anti_join(point_count, acoustic_data, by = c(&quot;site_id&quot;, &quot;date&quot;)) %&gt;% dplyr::select(site_id, date) %&gt;% distinct() # remove data from sites and visits from the point count data which do not have corresponding acoustic annotations point_count &lt;- point_count %&gt;% anti_join(dates_to_remove, by = c(&quot;site_id&quot;, &quot;date&quot;)) # as a sanity check, let&#39;s repeat the above with acoustic data to ensure we are not keeping any extra site/visit annotations acoustic_data_to_remove &lt;- anti_join(acoustic_data, point_count, by = c(&quot;site_id&quot;, &quot;date&quot;)) %&gt;% dplyr::select(site_id, date) %&gt;% distinct() # sanity check successful and suggests that the acoustic annotations are often across dates and visits that have point_count data. No data needs to be further filtered. # combine the datasets datSubset &lt;- bind_rows(point_count[,c(1:3,8,9,11,12,15,18:21)], acoustic_data[,c(4:9,15,16,18:21)]) # add a site name column datSubset &lt;- datSubset %&gt;% mutate(site_name = case_when( grepl(&quot;^ACAD&quot;, site_id) ~ &quot;Acadia National Park&quot;, grepl(&quot;^HBEF&quot;, site_id) ~ &quot;Hubbard Brook Experimental Forest&quot;, grepl(&quot;^KAWW&quot;, site_id) ~ &quot;Katahdin Woods and Waters&quot;, grepl(&quot;^MABI&quot;, site_id) ~ &quot;Marsh-Billings-Rockefeller NHP&quot; )) # visit_numbers need to be reassigned and sorted based on the earliest date for each site_id # this is being done as a secondary sanity check to ensure that the same visit_numbers are being assigned to every single site_id and date combination for each data_type. In other words, for MABI1110 for say 2022-05-26 with visit_number 1 for point_count data, we ensure that the visit_number is 1 for the same date and site_id for acoustic data datSubset &lt;- datSubset %&gt;% group_by(site_id, data_type) %&gt;% arrange(date) %&gt;% mutate(visit_number = dense_rank(date)) # write to file write.csv(datSubset, &quot;results/pooled_pointCount_acoustic_data.csv&quot;, row.names = F) "],["subsetting-data-and-running-exploratory-data-analyses.html", "Section 3 Subsetting data and running exploratory data analyses 3.1 Load necessary libraries 3.2 Load dataframe containing point count and acoustic data 3.3 Estimate total abundance and richness for point count and acoustic data 3.4 Creating a single dataframe prior to running statistical analysis 3.5 Filtering criteria for species inclusion 3.6 Explore whether time of day shows any variation in abundance/detections? 3.7 Examining abundance and acoustic detections as a function of time of day by species 3.8 Save the subsetted data to file", " Section 3 Subsetting data and running exploratory data analyses In this script, we will compute species richness, analyze data by time of data and subset data by including only species that occur across x% of sites for further analyses. 3.1 Load necessary libraries library(tidyverse) library(dplyr) library(stringr) library(vegan) library(scico) library(data.table) library(extrafont) library(sf) library(raster) library(scales) library(ggplot2) library(ggspatial) library(colorspace) library(lubridate) library(naniar) library(ggstatsplot) library(MetBrewer) library(gridExtra) library(grid) 3.2 Load dataframe containing point count and acoustic data datSubset &lt;- read.csv(&quot;results/pooled_pointCount_acoustic_data.csv&quot;) 3.3 Estimate total abundance and richness for point count and acoustic data ## point-count data # estimate total abundance across all species for each site # note: this analysis ignores visit_level information and pools overall abundance across visits per year to a site abundance &lt;- datSubset %&gt;% filter(data_type == &quot;point_count&quot;) %&gt;% group_by(site_id, site_name, year, scientific_name, common_name) %&gt;% summarise(total_abundance = sum(abundance)) %&gt;% ungroup() # Red Crossbill in Acadia National Park (ACAD3403) in year 2023 and Black-throated Green Warbler in year 2022 in Acadia National Park (ACAD3504) reported highest values of abundance (at n= 19 and n = 16, respectively) # estimate richness for point_count data at each site pc_richness &lt;- abundance %&gt;% mutate(forRichness = case_when(total_abundance &gt; 0 ~ 1)) %&gt;% group_by(site_id, site_name, year) %&gt;% summarise(richness = sum(forRichness)) %&gt;% mutate(data_type = &quot;point_count&quot;) %&gt;% ungroup() # while this analysis is not particularly informative, it does tell us that KAWW5401 in year 2023 reported highest species richness (value=30) # please note that both the calculations above discount the fact that there is an uneven number of visits to each site across years # estimate total number of detections using the manual annotations of the acoustic data # we ignore visit_level information and pool total detections at each site per year for this analysis detections &lt;- datSubset %&gt;% filter(data_type == &quot;acoustic_data&quot;) %&gt;% group_by(site_id, year, site_name, scientific_name, common_name) %&gt;% summarise(total_detections = n()) %&gt;% ungroup() # while this information is pooled across visits, the analysis reported highest detections for the Chestnut-sided Warbler at MABI1103 in Marsh-Billings in 2023 (n= 256 detections) # estimate richness for acoustic data aru_richness &lt;- detections %&gt;% mutate(forRichness = case_when(total_detections &gt; 0 ~ 1)) %&gt;% group_by(site_id, site_name, year) %&gt;% summarise(richness = sum(forRichness)) %&gt;% mutate(data_type = &quot;acoustic_data&quot;) %&gt;% ungroup() # while this analysis is not particularly informative, it does tell us that MABI1102 in year 2022 reported highest species richness (value= 23) # how many species were detected in a point count but not acoustic data? pc_not_aru &lt;- unique(anti_join(abundance[,5], detections[,5])) # Nine species were detected only in point count data but not in the acoustic data and can be ignored for downstream analysis: Herring Gull, Northern Cardinal, American Goshawk, Swamp Sparrow, Red-bellied Woodpecker, Pine Siskin, Savannah Sparrow, Eastern Kingbird, Brown-headed Cowbird # how many species were detected in acoustic data but not point counts? aru_not_pc &lt;- unique(anti_join(detections[,5], abundance[,5])) # Six species were detected only in acoustic data and not in the point count data and can also be ignored for downstream analysis: Cape May Warbler, Gray Catbird, Mallard, Prairie Warbler, House Wren, Bobolink 3.4 Creating a single dataframe prior to running statistical analysis ## filter out species that were only detected in one approach and not in the other point_count &lt;- datSubset %&gt;% filter(data_type == &quot;point_count&quot;) %&gt;% filter(!common_name %in% c(&quot;Herring Gull&quot;, &quot;Northern Cardinal&quot;, &quot;American Goshawk&quot;, &quot;Swamp Sparrow&quot;, &quot;Red-bellied Woodpecker&quot;, &quot;Pine Siskin&quot;, &quot;Savannah Sparrow&quot;, &quot;Eastern Kingbird&quot;, &quot;Brown-headed Cowbird&quot; )) acoustic_data &lt;- datSubset %&gt;% filter(data_type == &quot;acoustic_data&quot;) %&gt;% filter(!common_name %in% c(&quot;Cape May Warbler&quot;, &quot;Gray Catbird&quot;, &quot;Mallard&quot;, &quot;Prairie Warbler&quot;, &quot;House Wren&quot;, &quot;Bobolink&quot; )) # estimate abundance for each species for each site/visit combination # ignore distance for now as we can model it in future analysis abundance &lt;- point_count %&gt;% group_by(site_id, site_name, year, start_time, visit_number, scientific_name, common_name) %&gt;% summarise(abundance = sum(abundance)) %&gt;% ungroup() # estimate detections for each species for each site and year combination using the manual annotations of the acoustic data detections &lt;- acoustic_data %&gt;% group_by(site_id, site_name, year, visit_number, scientific_name, common_name) %&gt;% summarise(detections = n()) %&gt;% ungroup() ## sanity check to ensure that we are not including any species in either data type by mistake pc_not_aru &lt;- unique(anti_join(abundance[,7], detections[,6])) aru_not_pc &lt;- unique(anti_join(detections[,6], abundance[,7])) 3.5 Filtering criteria for species inclusion How do we decide on the number of species to include or what species to keep? One criteria that was employed in Ramesh et al. 2022 (Ecography) was: “we kept only those species that occurred in at least 5% of all checklists across at least 27 unique grid cells (50% of the study area).” In a similar vein, we keep species occurring in atleast 15% of sites across both years for both acoustic data and point count data. While the percentage is arbitrary, this number reflects an objective approach/criteria to filter/keep species. # add a total number of unique sites # 104 unique sites total_sites &lt;- length(unique(abundance$site_id)) # how many sites was each species observed in when carrying out point counts and when carrying out manual acoustic annotations/detections? species_year_pc &lt;- abundance %&gt;% group_by(common_name, year) %&gt;% summarise(n_sites = n_distinct(site_id), .groups = &#39;drop&#39;) %&gt;% arrange(desc(n_sites)) %&gt;% mutate(total_sites, prop_sites = n_sites/total_sites) # proportion of sites species_year_aru &lt;- detections %&gt;% group_by(common_name, year) %&gt;% summarise(n_sites = n_distinct(site_id), .groups = &#39;drop&#39;) %&gt;% arrange(desc(n_sites)) %&gt;% mutate(total_sites, prop_sites = n_sites/total_sites) # proportion of sites # we choose to keep species that occurred in atleast 15% of sites in each method and within a year species_pc &lt;- species_year_pc %&gt;% filter(prop_sites &gt;= 0.15) species_aru &lt;- species_year_aru %&gt;% filter(prop_sites &gt;= 0.15) # subset data from the abundance and detections dataframe to keep only species that occur in 15% of all sites during each year of survey aru_subset &lt;- detections %&gt;% semi_join(species_aru, by = c(&quot;common_name&quot;, &quot;year&quot;)) pc_subset &lt;- abundance %&gt;% semi_join(species_pc, by = c(&quot;common_name&quot;, &quot;year&quot;)) # we only keep species that satisfied the criteria for both point counts and acoustic data. In some cases, a few species satisfied the criteria for only point counts or only acoustic data and such species were filtered out prior to other analyses # how many species were detected in acoustic data but not point counts? # no new species were matching the criteria in acoustic data aru_not_pc &lt;- unique(anti_join(aru_subset[,6], pc_subset[,7])) # five species matched criteria in point counts that were not found in the acoustic data pc_not_aru &lt;- unique(anti_join(pc_subset[,7], aru_subset[,6])) # removing these five species from the pc_subset pc_subset &lt;- pc_subset %&gt;% filter(!common_name %in% c(&quot;Dark-eyed Junco&quot;,&quot;Red Crossbill&quot;, &quot;American Goldfinch&quot;,&quot;Wood Thrush&quot;, &quot;American Redstart&quot;)) # create a single dataframe data &lt;- full_join(pc_subset, aru_subset) %&gt;% replace_na(list(abundance = 0, detections = 0)) # criteria of 15% occurrence across sites per year/season for both acoustic data and point count data resulted in 22 species. 3.6 Explore whether time of day shows any variation in abundance/detections? Prior to running statistical models comparing abundance and acoustic detections, we explore the two aforementioned variables as a function of time of day of survey/detection. # examine summary of the data and check for NAs miss_var_summary(data) # start_time column has NAs which is mostly showing up for the acoustic data suggesting that there are species reported in the audio data during that visit that was not detected in that point count for the very same corresponding visit data &lt;- data %&gt;% group_by(site_id, year, visit_number) %&gt;% fill(start_time, .direction = &quot;downup&quot;) %&gt;% ungroup() 3.7 Examining abundance and acoustic detections as a function of time of day by species # looking at time as a continuous value, and the counts as the estimate of abundance data |&gt; mutate(start_time = hms::as_hms(start_time)) |&gt; ggplot(aes(x = start_time, y = abundance, color = site_name)) + geom_segment(aes(y = 0, yend = abundance), alpha = 0.3) + geom_point(alpha = 0.6) + facet_wrap(~common_name, ncol = 3) + labs(x = &quot;Time&quot;, y = &quot;Number of individuals counted in a 10-minute point count&quot;, color = &quot;Site&quot;)+ scale_color_manual(values=met.brewer(&quot;Homer2&quot;, 4))+ theme_minimal()+ theme(legend.position = &quot;bottom&quot;); ggsave(filename = &quot;figs/abundance-by-time-continuous.jpg&quot;, width = 10, height = 10, units = &quot;in&quot;, dpi = 300) data |&gt; mutate(start_time = hms::as_hms(start_time)) |&gt; ggplot(aes(x = start_time, y = detections, color = site_name)) + geom_segment(aes(y = 0, yend = detections), alpha = 0.3) + geom_point(alpha = 0.6) + facet_wrap(~common_name, ncol = 3, scale = &quot;free_y&quot;) + scale_color_manual(values=met.brewer(&quot;Homer2&quot;, 4))+ labs(x = &quot;Time&quot;, y = &quot;Number of acoustic detections in a 10-minute recording&quot;, color = &quot;Site&quot;)+ theme_minimal()+ theme(legend.position = &quot;bottom&quot;) ggsave(filename = &quot;figs/acu_detections-by-time-continuous.jpg&quot;, width = 10, height = 10, units = &quot;in&quot;, dpi = 300) # I (Orlando) will play with this `data` object saveRDS(data, &quot;data/data_time_continuous.RDS&quot;) Interestingly, we are not observing much variation between times of day for both acoustic detections and species abundances across species, potentially suggesting that time of day may/may not be impacting our results or might contribute to differences we are observing. 3.8 Save the subsetted data to file # write to file write.csv(data, &quot;results/datSubset.csv&quot;, row.names = F) "],["creating-study-area-map.html", "Section 4 Creating study area map 4.1 Load necessary libraries 4.2 Load associated shapefiles for each region sampled 4.3 Download natural earth layers 4.4 Acadia National Park 4.5 Hubbard Brook 4.6 Katahdin Woods and Waters 4.7 Marsh Billings", " Section 4 Creating study area map Here, I will create a map featuring locations of the point counts and acoustic data. 4.1 Load necessary libraries library(tidyverse) library(dplyr) library(stringr) library(vegan) library(scico) library(data.table) library(extrafont) library(sf) library(raster) library(scales) library(ggplot2) library(ggspatial) library(colorspace) library(lubridate) library(naniar) library(ggstatsplot) library(MetBrewer) library(FedData) library(patchwork) library(rnaturalearth) library(tmap) library(ggspatial) 4.2 Load associated shapefiles for each region sampled # load sites sampled points &lt;- read.csv(&quot;data/list-of-sites.csv&quot;) %&gt;% st_as_sf(coords = c(&quot;longitude&quot;, &quot;latitude&quot;), crs = 4326) # add a site name column points &lt;- points %&gt;% mutate(site_name = case_when( grepl(&quot;^ACAD&quot;, site_id) ~ &quot;Acadia National Park&quot;, grepl(&quot;^HBEF&quot;, site_id) ~ &quot;Hubbard Brook Experimental Forest&quot;, grepl(&quot;^KAWW&quot;, site_id) ~ &quot;Katahdin Woods and Waters&quot;, grepl(&quot;^MABI&quot;, site_id) ~ &quot;Marsh-Billings-Rockefeller NHP&quot; )) # load shapefiles for Acadia National Park, Marsh-Billings, Katahdin and Hubbard Brook # this data was accessed from several federal sources and using an R package # Acadia National Park acadia &lt;- get_padus(template = &quot;Acadia National Park&quot;, label = &quot;Acadia&quot;) acadia &lt;- acadia$Manager_Name[acadia$Manager_Name$Des_Tp == &quot;NP&quot;,] #subset required polygons after inspecting # convert to multiple polygons to remove the boundary polygons acadia &lt;- st_cast(acadia, &quot;POLYGON&quot;) acadia &lt;- acadia[-c(1:5), ] # hubbard brook hubbard_brook &lt;- st_read(&quot;data/spatial/hbef_boundary.shp&quot;) %&gt;% st_transform(., crs = 4326) # katahdin katahdin &lt;- st_read(&quot;data/spatial/nps_boundary.shp&quot;) %&gt;% filter(UNIT_NAME == &quot;Katahdin Woods and Waters National Monument&quot;) %&gt;% st_transform(., crs = 4326) # marsh billings marsh_billings &lt;- get_padus(template = &quot;Marsh-Billings-Rockefeller National Historical Park&quot;,label = &quot;MBR&quot;) marsh_billings &lt;- marsh_billings$Manager_Name[marsh_billings$Manager_Name$Own_Name == &quot;NPS&quot;,] 4.3 Download natural earth layers Let’s download layers that we can add to the map to make it look a little pretty ne_shapes_to_get &lt;- c( &quot;rivers_lake_centerlines&quot;, &quot;rivers_north_america&quot;, &quot;lakes&quot;, &quot;lakes_north_america&quot; ) # loop through ne_shapes_to_get and download each shapefile and store it locally ne_shapes_to_get %&gt;% walk(~ ne_download( scale = 10, type = .x, category = &quot;physical&quot;, destdir = &quot;data/spatial/&quot;, load = FALSE )) # load each pre-downloaded shapefile and store it in a list ne_data_list &lt;- ne_shapes_to_get %&gt;% map(~ { ne_load( scale = 10, type = .x, category = &quot;physical&quot;, destdir = &quot;data/spatial/&quot;, returnclass = &quot;sf&quot; ) %&gt;% st_transform(., crs = 4326) }) %&gt;% set_names(ne_shapes_to_get) # load all the datasets in the list into the global environment list2env(ne_data_list, envir = .GlobalEnv) 4.4 Acadia National Park # Acadia National Park acadia_points &lt;- points %&gt;% filter(site_name == &quot;Acadia National Park&quot;) # get data for Northeast US usa &lt;- ne_states(country = &quot;United States of America&quot;, returnclass = &quot;sf&quot;) northeast_states &lt;- c(&quot;Maine&quot;, &quot;New Hampshire&quot;, &quot;Vermont&quot;, &quot;Massachusetts&quot;, &quot;Rhode Island&quot;, &quot;Connecticut&quot;, &quot;New York&quot;, &quot;New Jersey&quot;, &quot;Pennsylvania&quot;) northeast &lt;- usa[usa$name %in% northeast_states, ] maine &lt;- usa[usa$name == &quot;Maine&quot;, ] # get approximate bounding box of Acadia for inset highlight acadia_bbox &lt;- st_bbox(acadia) acadia_point &lt;- st_centroid(st_as_sfc(acadia_bbox)) # create inset map for Acadia inset_acadia &lt;- ggplot() + geom_sf(data = northeast, fill = &quot;grey90&quot;, color = &quot;white&quot;, size = 0.1) + geom_sf(data = maine, fill = &quot;grey50&quot;, color = &quot;white&quot;, size = 0.2) + geom_sf(data = acadia_point, color = &quot;black&quot;, size = 2.5) + coord_sf(crs = 4326, xlim = c(-80, -66), ylim = c(40, 48)) + theme_void() + theme( panel.background = element_rect(fill = &quot;white&quot;, color = &quot;black&quot;, size = 0.5), plot.background = element_rect(fill = &quot;white&quot;, color = &quot;black&quot;, size = 0.5) ) # main plot plot_acadia &lt;- ggplot() + geom_sf(data = acadia, fill = &quot;#FFE6B7FF&quot;) + geom_sf(data = acadia_points, size = 1.2, color = &quot;#de2d26&quot;) + coord_sf(crs = 4326) + theme_bw(base_family = &quot;Century Gothic&quot;) + annotation_north_arrow( location = &quot;tr&quot;, pad_x = unit(0.3, &quot;cm&quot;), pad_y = unit(0.3, &quot;cm&quot;), height = unit(0.8, &quot;cm&quot;), width = unit(0.8, &quot;cm&quot;), style = north_arrow_fancy_orienteering(fill = c(&quot;grey30&quot;, &quot;white&quot;), line_col = &quot;grey30&quot;, text_family = &quot;Century Gothic&quot;, text_size = 8)) + annotation_scale( location = &quot;bl&quot;, pad_x = unit(0.1, &quot;cm&quot;), pad_y = unit(0.2, &quot;cm&quot;), height = unit(0.3, &quot;cm&quot;), bar_cols = c(&quot;grey30&quot;, &quot;white&quot;), text_family = &quot;Century Gothic&quot;, text_cex = 0.6, unit_category = &quot;metric&quot;) + labs(title = &quot;Acadia National Park&quot;) # position the inset in bottom right acadia_with_inset &lt;- plot_acadia + annotation_custom( ggplotGrob(inset_acadia), xmin = st_bbox(acadia)[3] - (st_bbox(acadia)[3] - st_bbox(acadia)[1]) * 0.20, xmax = st_bbox(acadia)[3], ymin = st_bbox(acadia)[2], ymax = st_bbox(acadia)[2] + (st_bbox(acadia)[4] - st_bbox(acadia)[2]) * 0.30 ) acadia_with_inset 4.5 Hubbard Brook # hubbard brook hb_points &lt;- points %&gt;% filter(site_name == &quot;Hubbard Brook Experimental Forest&quot;) # get data for Northeast US new_hampshire &lt;- usa[usa$name == &quot;New Hampshire&quot;, ] # get approximate bounding box of Hubbard Brook for inset highlight hb_bbox &lt;- st_bbox(hubbard_brook) hb_point &lt;- st_centroid(st_as_sfc(hb_bbox)) # create inset map for Acadia inset_hb &lt;- ggplot() + geom_sf(data = northeast, fill = &quot;grey90&quot;, color = &quot;white&quot;, size = 0.1) + geom_sf(data = new_hampshire,fill = &quot;grey50&quot;, color = &quot;white&quot;, size = 0.2) + geom_sf(data = hb_point, color = &quot;black&quot;, size = 2.5) + coord_sf(crs = 4326, xlim = c(-80, -66), ylim = c(40, 48)) + theme_void() + theme( panel.background = element_rect(fill = &quot;white&quot;, color = &quot;black&quot;, size = 0.5), plot.background = element_rect(fill = &quot;white&quot;, color = &quot;black&quot;, size = 0.5) ) # main plot plot_hb &lt;- ggplot() + geom_sf(data = hubbard_brook, fill = &quot;#FFE6B7FF&quot;) + geom_sf(data = hb_points, size = 1.2, color = &quot;#de2d26&quot;) + coord_sf(crs = 4326) + theme_bw(base_family = &quot;Century Gothic&quot;) + annotation_north_arrow( location = &quot;tr&quot;, pad_x = unit(0.3, &quot;cm&quot;), pad_y = unit(0.3, &quot;cm&quot;), height = unit(0.8, &quot;cm&quot;), width = unit(0.8, &quot;cm&quot;), style = north_arrow_fancy_orienteering(fill = c(&quot;grey30&quot;, &quot;white&quot;), line_col = &quot;grey30&quot;, text_family = &quot;Century Gothic&quot;, text_size = 8)) + annotation_scale( location = &quot;bl&quot;, pad_x = unit(0.1, &quot;cm&quot;), pad_y = unit(0.2, &quot;cm&quot;), height = unit(0.3, &quot;cm&quot;), bar_cols = c(&quot;grey30&quot;, &quot;white&quot;), text_family = &quot;Century Gothic&quot;, text_cex = 0.6, unit_category = &quot;metric&quot;) + labs(title = &quot;Hubbard Brook Experimental Forest&quot;) # position the inset in bottom right hb_with_inset &lt;- plot_hb + annotation_custom( ggplotGrob(inset_hb), xmin = st_bbox(hubbard_brook)[3] - (st_bbox(hubbard_brook)[3] - st_bbox(hubbard_brook)[1]) * 0.20, xmax = st_bbox(hubbard_brook)[3], ymin = st_bbox(hubbard_brook)[2], ymax = st_bbox(hubbard_brook)[2] + (st_bbox(hubbard_brook)[4] - st_bbox(hubbard_brook)[2]) * 0.30 ) hb_with_inset 4.6 Katahdin Woods and Waters # katahdin points katahdin_points &lt;- points %&gt;% filter(site_name == &quot;Katahdin Woods and Waters&quot;) # get approximate bounding box of Katahdin for inset highlight katahdin_bbox &lt;- st_bbox(katahdin) katahdin_point &lt;- st_centroid(st_as_sfc(katahdin_bbox)) # create inset map for Acadia inset_katahdin &lt;- ggplot() + geom_sf(data = northeast, fill = &quot;grey90&quot;, color = &quot;white&quot;, size = 0.1) + geom_sf(data = maine,fill = &quot;grey50&quot;, color = &quot;white&quot;, size = 0.2) + geom_sf(data = katahdin_point, color = &quot;black&quot;, size = 2.5) + coord_sf(crs = 4326, xlim = c(-80, -66), ylim = c(40, 48)) + theme_void() + theme( panel.background = element_rect(fill = &quot;white&quot;, color = &quot;black&quot;, size = 0.5), plot.background = element_rect(fill = &quot;white&quot;, color = &quot;black&quot;, size = 0.5) ) # main plot plot_katahdin &lt;- ggplot() + geom_sf(data = katahdin, fill = &quot;#FFE6B7FF&quot;) + geom_sf(data = katahdin_points, size = 1.2, color = &quot;#de2d26&quot;) + coord_sf(crs = 4326) + theme_bw(base_family = &quot;Century Gothic&quot;) + annotation_north_arrow( location = &quot;tr&quot;, pad_x = unit(0.3, &quot;cm&quot;), pad_y = unit(0.3, &quot;cm&quot;), height = unit(0.8, &quot;cm&quot;), width = unit(0.8, &quot;cm&quot;), style = north_arrow_fancy_orienteering(fill = c(&quot;grey30&quot;, &quot;white&quot;), line_col = &quot;grey30&quot;, text_family = &quot;Century Gothic&quot;, text_size = 8)) + annotation_scale( location = &quot;bl&quot;, pad_x = unit(0.1, &quot;cm&quot;), pad_y = unit(0.2, &quot;cm&quot;), height = unit(0.3, &quot;cm&quot;), bar_cols = c(&quot;grey30&quot;, &quot;white&quot;), text_family = &quot;Century Gothic&quot;, text_cex = 0.6, unit_category = &quot;metric&quot;) + labs(title = &quot;Katahdin Woods and Waters&quot;) # position the inset in bottom right katahdin_with_inset &lt;- plot_katahdin + annotation_custom( ggplotGrob(inset_katahdin), xmin = st_bbox(katahdin)[3] - (st_bbox(katahdin)[3] - st_bbox(katahdin)[1]) * 0.20, xmax = st_bbox(katahdin)[3], ymin = st_bbox(katahdin)[2], ymax = st_bbox(katahdin)[2] + (st_bbox(katahdin)[4] - st_bbox(katahdin)[2]) * 1.85 ) katahdin_with_inset 4.7 Marsh Billings # save ggsave(acadia_with_inset, filename = &quot;figs/acadia.png&quot;, width = 12, height = 7, device = png(), units = &quot;in&quot;, dpi = 300) ggsave(hb_with_inset, filename = &quot;figs/hb.png&quot;, width = 12, height = 7, device = png(), units = &quot;in&quot;, dpi = 300) ggsave(katahdin_with_inset, filename = &quot;figs/katahdin.png&quot;, width = 12, height = 7, device = png(), units = &quot;in&quot;, dpi = 300) dev.off() ggsave(mbi_with_inset, filename = &quot;figs/marshbil.png&quot;, width = 12, height = 7, device = png(), units = &quot;in&quot;, dpi = 300) dev.off() # Acadia National Park mbi_points &lt;- points %&gt;% filter(site_name == &quot;Marsh-Billings-Rockefeller NHP&quot;) # get data for Northeast US vermont &lt;- usa[usa$name == &quot;Vermont&quot;, ] # get approximate bounding box of Acadia for inset highlight mbi_bbox &lt;- st_bbox(marsh_billings) mbi_point &lt;- st_centroid(st_as_sfc(mbi_bbox)) # create inset map for Acadia inset_mbi &lt;- ggplot() + geom_sf(data = northeast, fill = &quot;grey90&quot;, color = &quot;white&quot;, size = 0.1) + geom_sf(data = vermont, fill = &quot;grey50&quot;, color = &quot;white&quot;, size = 0.2) + geom_sf(data = mbi_point, color = &quot;black&quot;, size = 2.5) + coord_sf(crs = 4326, xlim = c(-80, -66), ylim = c(40, 48)) + theme_void() + theme( panel.background = element_rect(fill = &quot;white&quot;, color = &quot;black&quot;, size = 0.5), plot.background = element_rect(fill = &quot;white&quot;, color = &quot;black&quot;, size = 0.5) ) # main plot plot_mbi &lt;- ggplot() + geom_sf(data = marsh_billings, fill = &quot;#FFE6B7FF&quot;) + geom_sf(data = mbi_points, size = 1.2, color = &quot;#de2d26&quot;) + coord_sf(crs = 4326) + theme_bw(base_family = &quot;Century Gothic&quot;) + annotation_north_arrow( location = &quot;tr&quot;, pad_x = unit(0.3, &quot;cm&quot;), pad_y = unit(0.3, &quot;cm&quot;), height = unit(0.8, &quot;cm&quot;), width = unit(0.8, &quot;cm&quot;), style = north_arrow_fancy_orienteering(fill = c(&quot;grey30&quot;, &quot;white&quot;), line_col = &quot;grey30&quot;, text_family = &quot;Century Gothic&quot;, text_size = 8)) + annotation_scale( location = &quot;bl&quot;, pad_x = unit(0.1, &quot;cm&quot;), pad_y = unit(0.2, &quot;cm&quot;), height = unit(0.3, &quot;cm&quot;), bar_cols = c(&quot;grey30&quot;, &quot;white&quot;), text_family = &quot;Century Gothic&quot;, text_cex = 0.6, unit_category = &quot;metric&quot;) + labs(title = &quot;Marsh-Billings-Rockefeller NHP&quot;) # position the inset in bottom right mbi_with_inset &lt;- plot_mbi + annotation_custom( ggplotGrob(inset_mbi), xmin = st_bbox(marsh_billings)[3] - (st_bbox(marsh_billings)[3] - st_bbox(marsh_billings)[1]) * 0.20, xmax = st_bbox(marsh_billings)[3], ymin = st_bbox(marsh_billings)[2], ymax = st_bbox(marsh_billings)[2] + (st_bbox(marsh_billings)[4] - st_bbox(marsh_billings)[2]) * 1.60 ) mbi_with_inset "],["abundance-vs.-acoustic-detections.html", "Section 5 Abundance vs. acoustic detections 5.1 Load necessary libraries 5.2 Load dataframe containing data that is already filtered for a subset of species 5.3 Correlations between abundance and detections 5.4 Scaling of predictors and determining distribution family 5.5 Generalized linear mixed model between abundance and acoustic detections 5.6 Model diagnostics 5.7 Predict abundance given detections", " Section 5 Abundance vs. acoustic detections In this script, we will associations between abundance as estimated from point count data and acoustic detections, which were manually annotated from simultaneously paired audio recorders. 5.1 Load necessary libraries library(tidyverse) library(dplyr) library(stringr) library(vegan) library(scico) library(data.table) library(extrafont) library(sf) library(raster) library(scales) library(ggplot2) library(ggspatial) library(colorspace) library(lubridate) library(naniar) library(patchwork) library(glmmTMB) library(DHARMa) library(performance) library(effects) library(parallel) library(foreach) library(doParallel) library(gridExtra) library(broom.mixed) library(scales) 5.2 Load dataframe containing data that is already filtered for a subset of species data &lt;- read.csv(&quot;results/datSubset.csv&quot;) 5.3 Correlations between abundance and detections # explore the distribution of data hist(data$abundance, main = &quot;Abundance Distribution&quot;, xlab = &quot;Abundance&quot;) # most species had an abundance of 1, followed by 2 and fewer species had higher abundances and so on hist(data$detections, main = &quot;Acoustic Detections Distribution&quot;, xlab = &quot;Detections&quot;) # right-tailed skewed distribution that is zero-inflated plot(data$detections, data$abundance, xlab = &quot;Acoustic Detections&quot;, ylab = &quot;Abundance&quot;) # Check correlation cor(data$abundance, data$detections, use = &quot;complete.obs&quot;) # 0.324 is the value of correlation between abundance and detections across species, sites and years. 5.4 Scaling of predictors and determining distribution family Since it’s abundance, we will most likely choose Poisson, but we can do a quick assessment of the distribution to ensure we are choosing the right family prior to running mixed models. ## we scale acoustic detections since the range is vastly different when compared to abundance (0-10 vs. 1-500 for example) data$detections_scaled &lt;- scale(data$detections) # analyze abundance distribution to choose appropriate family abundance_analysis &lt;- function(y) { # summary statistics stats &lt;- list( mean = mean(y, na.rm = TRUE), var = var(y, na.rm = TRUE), min = min(y, na.rm = TRUE), max = max(y, na.rm = TRUE), zeros = sum(y == 0, na.rm = TRUE), total_obs = length(y[!is.na(y)]) ) # dispersion ratio (variance/mean) stats$dispersion_ratio &lt;- stats$var / stats$mean # proportion of zeros stats$zero_prop &lt;- stats$zeros / stats$total_obs return(stats) } abundance_stats &lt;- abundance_analysis(data$abundance) print(abundance_stats) # visualize to confirm hist(data$abundance, main = &quot;Abundance Distribution&quot;) plot(table(data$abundance), main = &quot;Abundance Frequency&quot;) qqnorm(data$abundance) qqline(data$abundance) # the data suggests that the response is underdispersed (variance is less than the mean and while the data is counts, we rely on a modified version of the Poisson distribution: the Conway-Maxwell-Poisson family that handles both under and overdispersion. 5.5 Generalized linear mixed model between abundance and acoustic detections # ensure factors are properly coded data$common_name &lt;- as.factor(data$common_name) data$year &lt;- as.factor(data$year) data$site_id &lt;- as.factor(data$site_id) # recode visit_number as year_visit_number to indicate repeat visits are different between years data &lt;- data %&gt;% mutate(year_visit_number = paste(year, visit_number, sep = &quot;_&quot;)) # set up parallel processing n_cores &lt;- detectCores() - 1 # Leave one core free cl &lt;- makeCluster(n_cores) registerDoParallel(cl) # export necessary objects to clusters clusterEvalQ(cl, { library(glmmTMB) }) clusterExport(cl, c(&quot;data&quot;)) # we will run models separately for each species model_results &lt;- list() # include error handling for species for which the models failed failed_species &lt;- character() # get unique species species_list &lt;- unique(data$common_name) for(species in species_list) { cat(&quot;Fitting model for:&quot;, species, &quot;\\n&quot;) species_data &lt;- data[data$common_name == species, ] # skip if not enough data if(nrow(species_data) &lt; 10) { cat(&quot;Skipping&quot;, species, &quot;- insufficient data\\n&quot;) next } tryCatch({ model_results[[species]] &lt;- glmmTMB( abundance ~ detections_scaled + (1|site_id) + (1|year_visit_number), data = species_data, family = compois, control = glmmTMBControl(parallel = n_cores, optCtrl = list(iter.max = 1000, eval.max = 1000)) ) }, error = function(e) { cat(&quot;Model failed for&quot;, species, &quot;:&quot;, e$message, &quot;\\n&quot;) failed_species &lt;&lt;- c(failed_species, species) }) } # check which species failed print(failed_species) # no models failed but some model/models did not converge # are there models that did not converge? converged_status &lt;- sapply(names(model_results), function(species) { model &lt;- model_results[[species]] if(inherits(model, &quot;glmmTMB&quot;)) { return(model$fit$convergence == 0) } return(TRUE) }) # show which species didn&#39;t converge non_converged &lt;- names(converged_status)[!converged_status] cat(&quot;Species that didn&#39;t converge:&quot;, non_converged, &quot;\\n&quot;) # we can get rid of Scarlet Tanager from the next set of analyses 5.6 Model diagnostics # remove non-converged models from model_results # we got rid of Scarlet Tanager if(length(non_converged) &gt; 0) { model_results &lt;- model_results[!names(model_results) %in% non_converged] cat(&quot;Removed&quot;, length(non_converged), &quot;non-converged models\\n&quot;) } # model diagnostics using DHARMa R package diagnostics_results &lt;- list() cat(&quot;Running model diagnostics...\\n&quot;) for(species in names(model_results)) { cat(&quot;Diagnostics for:&quot;, species, &quot;\\n&quot;) tryCatch({ # create DHARMa residuals simulationOutput &lt;- simulateResiduals(fittedModel = model_results[[species]], plot = FALSE) # store diagnostics diagnostics_results[[species]] &lt;- list( simulation = simulationOutput, uniformity_test = testUniformity(simulationOutput, plot = FALSE), dispersion_test = testDispersion(simulationOutput, plot = FALSE), outliers_test = testOutliers(simulationOutput, plot = FALSE) ) # print test results cat(&quot; Uniformity test p-value:&quot;, diagnostics_results[[species]]$uniformity_test$p.value, &quot;\\n&quot;) cat(&quot; Dispersion test p-value:&quot;, diagnostics_results[[species]]$dispersion_test$p.value, &quot;\\n&quot;) cat(&quot; Outliers test p-value:&quot;, diagnostics_results[[species]]$outliers_test$p.value, &quot;\\n&quot;) }, error = function(e) { cat(&quot;Diagnostics failed for&quot;, species, &quot;:&quot;, e$message, &quot;\\n&quot;) }) } # create diagnostic plots for all species pdf(&quot;figs/model-diagnostics.pdf&quot;, width = 12, height = 8) for(species in names(diagnostics_results)) { if(!is.null(diagnostics_results[[species]]$simulation)) { par(mfrow = c(2, 2)) plot(diagnostics_results[[species]]$simulation, main = paste(&quot;Diagnostics:&quot;, species)) } } dev.off() # no cases of overdispersion or mild issues of uniformity 5.7 Predict abundance given detections Following model diagnostic tests, we now plot abundance given acoustic detections for each species and save model summaries. # extract model fit statistics and effect sizes model_summary &lt;- list() effect_sizes &lt;- list() for(species in names(model_results)) { # extract model summary model_sum &lt;- summary(model_results[[species]]) # get fixed effects fixed_effects &lt;- tidy(model_results[[species]], effects = &quot;fixed&quot;) detection_effect &lt;- fixed_effects[fixed_effects$term == &quot;detections_scaled&quot;, ] # calculate R-squared (pseudo R-squared for GLMMs) # using correlation between fitted and observed values species_data &lt;- data[data$common_name == species, ] fitted_vals &lt;- fitted(model_results[[species]]) pseudo_r2 &lt;- cor(fitted_vals, species_data$abundance)^2 # store effect size and significance effect_sizes[[species]] &lt;- list( estimate = detection_effect$estimate, std_error = detection_effect$std.error, p_value = detection_effect$p.value, pseudo_r2 = pseudo_r2, significance = case_when( detection_effect$p.value &lt; 0.001 ~ &quot;***&quot;, detection_effect$p.value &lt; 0.01 ~ &quot;**&quot;, detection_effect$p.value &lt; 0.05 ~ &quot;*&quot;, detection_effect$p.value &lt; 0.1 ~ &quot;.&quot;, TRUE ~ &quot;ns&quot; ), direction = ifelse(detection_effect$estimate &gt; 0, &quot;Positive&quot;, &quot;Negative&quot;), effect_magnitude = case_when( abs(detection_effect$estimate) &gt; 1 ~ &quot;Large&quot;, abs(detection_effect$estimate) &gt; 0.5 ~ &quot;Medium&quot;, abs(detection_effect$estimate) &gt; 0.2 ~ &quot;Small&quot;, TRUE ~ &quot;Very Small&quot; ) ) } # get scaling parameters scaling_center &lt;- attr(data$detections_scaled, &quot;scaled:center&quot;) scaling_scale &lt;- attr(data$detections_scaled, &quot;scaled:scale&quot;) # create prediction data prediction_data_list &lt;- list() species_ecology_summary &lt;- list() for(species in names(model_results)) { species_data &lt;- data[data$common_name == species, ] # calculate ecological summary statistics species_ecology_summary[[species]] &lt;- list( n_observations = nrow(species_data), n_sites = length(unique(species_data$site_id)), mean_abundance = round(mean(species_data$abundance, na.rm = TRUE), 2), max_abundance = max(species_data$abundance, na.rm = TRUE), detection_range_original = range((species_data$detections_scaled * scaling_scale) + scaling_center), abundance_cv = round(sd(species_data$abundance, na.rm = TRUE) / mean(species_data$abundance, na.rm = TRUE), 2) ) # create prediction range based on observed data detections_range &lt;- seq(min(species_data$detections_scaled, na.rm = TRUE), max(species_data$detections_scaled, na.rm = TRUE), length.out = 100) typical_site &lt;- names(sort(table(species_data$site_id), decreasing = TRUE))[1] typical_visit &lt;- names(sort(table(species_data$year_visit_number), decreasing = TRUE))[1] pred_data &lt;- data.frame( detections_scaled = detections_range, site_id = typical_site, year_visit_number = as.factor(typical_visit), common_name = species ) # back-transform detections pred_data$detections_original &lt;- (pred_data$detections_scaled * scaling_scale) + scaling_center # generate predictions tryCatch({ predictions &lt;- predict(model_results[[species]], newdata = pred_data, type = &quot;response&quot;, se.fit = TRUE, re.form = NA) pred_data$predicted_abundance &lt;- predictions$fit pred_data$se &lt;- predictions$se.fit pred_data$lower_ci &lt;- pmax(0, predictions$fit - 1.96 * predictions$se.fit) pred_data$upper_ci &lt;- predictions$fit + 1.96 * predictions$se.fit # Add effect size information pred_data$effect_direction &lt;- effect_sizes[[species]]$direction pred_data$effect_magnitude &lt;- effect_sizes[[species]]$effect_magnitude pred_data$significance &lt;- effect_sizes[[species]]$significance pred_data$pseudo_r2 &lt;- effect_sizes[[species]]$pseudo_r2 pred_data$p_value &lt;- effect_sizes[[species]]$p_value # calculate fold change from min to max detections min_pred &lt;- pred_data$predicted_abundance[1] max_pred &lt;- pred_data$predicted_abundance[nrow(pred_data)] pred_data$fold_change &lt;- max_pred / min_pred prediction_data_list[[species]] &lt;- pred_data }, error = function(e) { cat(&quot;Prediction failed for&quot;, species, &quot;:&quot;, e$message, &quot;\\n&quot;) }) } # combine all prediction data all_predictions &lt;- do.call(rbind, prediction_data_list) # create observed data with original detection values observed_data_list &lt;- list() for(species in names(model_results)) { species_data &lt;- data[data$common_name == species, ] species_data$detections_original &lt;- (species_data$detections_scaled * scaling_scale) + scaling_center observed_data_list[[species]] &lt;- species_data } all_observed &lt;- do.call(rbind, observed_data_list) # create facet plot with statistical information fig_predict_det_abundance &lt;- ggplot() + # add observed data points with transparency geom_point(data = all_observed, aes(x = detections_original, y = abundance), alpha = 0.3, color = &quot;gray50&quot;, size = 1) + # add confidence interval geom_ribbon(data = all_predictions, aes(x = detections_original, ymin = lower_ci, ymax = upper_ci), alpha = 0.2, fill = &quot;steelblue&quot;) + # add prediction line colored by effect direction geom_line(data = all_predictions, aes(x = detections_original, y = predicted_abundance, color = effect_direction), size = 1.2) + # add statistical information as text geom_text(data = all_predictions %&gt;% group_by(common_name) %&gt;% summarise( detections_original = max(detections_original) * 0.05, predicted_abundance = max(predicted_abundance) * 0.95, significance = first(significance), pseudo_r2 = first(pseudo_r2), effect_magnitude = first(effect_magnitude), p_value = first(p_value), fold_change = first(fold_change), .groups = &#39;drop&#39; ), aes(x = detections_original, y = predicted_abundance, label = paste0(&quot;R² = &quot;, round(pseudo_r2, 3), significance )), hjust = 0, vjust = 1, size = 2.8, fontface = &quot;bold&quot;, color = &quot;black&quot;) + facet_wrap(~ common_name, scales = &quot;free&quot;, ncol = 3) + scale_color_manual(values = c(&quot;Positive&quot; = &quot;#2E8B57&quot;, &quot;Negative&quot; = &quot;#CD5C5C&quot;), name = &quot;Association&quot;) + labs( x = &quot;Number of acoustic detections&quot;, y = &quot;Predicted avian abundance (counts of individuals)&quot;, caption = &quot;R² = pseudo R-squared; *** p&lt;0.001, ** p&lt;0.01, * p&lt;0.05, . p&lt;0.1, ns = not significant\\nGray points = observed data, colored lines = model predictions ± 95% CI&quot; ) + theme_bw() + theme( strip.text = element_text(size = 10, face = &quot;bold&quot;, family = &quot;Century Gothic&quot;), axis.text = element_text(size = 9, family = &quot;Century Gothic&quot;), axis.title = element_text(size = 12, face = &quot;bold&quot;, family = &quot;Century Gothic&quot;), plot.title = element_text(size = 16, hjust = 0.5, face = &quot;bold&quot;, family = &quot;Century Gothic&quot;), plot.subtitle = element_text(size = 12, hjust = 0.5, face = &quot;italic&quot;, family = &quot;Century Gothic&quot;), plot.caption = element_text(size = 9, hjust = 0, family = &quot;Century Gothic&quot;), legend.position = &quot;bottom&quot;, panel.grid.minor = element_blank(), strip.background = element_rect(fill = &quot;lightgray&quot;) ) ggsave(&quot;figs/fig_predicted_avian_abundance_detections.png&quot;, fig_predict_det_abundance, width = 16, height = 12, dpi = 300,) dev.off() # create a summary table containing the model results summary_table &lt;- data.frame( Species = names(effect_sizes), N_Observations = sapply(species_ecology_summary, function(x) x$n_observations), N_Sites = sapply(species_ecology_summary, function(x) x$n_sites), Mean_Abundance = sapply(species_ecology_summary, function(x) x$mean_abundance), Max_Abundance = sapply(species_ecology_summary, function(x) x$max_abundance), Detection_Range = sapply(species_ecology_summary, function(x) paste(round(x$detection_range_original, 1), collapse = &quot; - &quot;)), Effect_Direction = sapply(effect_sizes, function(x) x$direction), Effect_Size = round(sapply(effect_sizes, function(x) x$estimate), 3), Effect_Magnitude = sapply(effect_sizes, function(x) x$effect_magnitude), P_Value = round(sapply(effect_sizes, function(x) x$p_value), 4), Significance = sapply(effect_sizes, function(x) x$significance), Pseudo_R2 = round(sapply(effect_sizes, function(x) x$pseudo_r2), 3), Fold_Change = round(sapply(prediction_data_list, function(x) x$fold_change[1]), 2) ) # write table to file write.csv(summary_table, &quot;results/glmm_model_abundance_detections.csv&quot;, row.names = FALSE) # write model results and filtered data to file for loading in future scripts save(model_results, file = &quot;results/glmm_model_results.RData&quot;) "],["calling-rates.html", "Section 6 Calling rates 6.1 Load necessary libraries 6.2 Load previous model object and filtered data 6.3 Calculating calling rate 6.4 Does calling rate vary as a function of number of individuals 6.5 Is calling rate linked to the fit between predicted abundance and acoustic detections?", " Section 6 Calling rates In this script, we compute calling rates and test if the relationship between predicted counts of individuals and acoustic detections is governed by calling activity. 6.1 Load necessary libraries library(tidyverse) library(dplyr) library(stringr) library(vegan) library(scico) library(data.table) library(extrafont) library(sf) library(raster) library(scales) library(ggplot2) library(ggspatial) library(colorspace) library(lubridate) library(naniar) library(patchwork) library(ggrepel) 6.2 Load previous model object and filtered data load(&quot;results/glmm_model_results.RData&quot;) summary_stats &lt;- read.csv(&quot;results/glmm_model_abundance_detections.csv&quot;) data &lt;- read.csv(&quot;results/datSubset.csv&quot;) # we will also load the unfiltered acoustic data and point count data for calling rate calculation unfiltered_data &lt;- read.csv(&quot;results/pooled_pointCount_acoustic_data.csv&quot;) 6.3 Calculating calling rate Here, we estimate species-specific calling rate as the detections per minute per species and the corresponding visit. # we work with the unfiltered data first to extract a detections per minute column for each species per visit detections_per_minute &lt;- unfiltered_data %&gt;% filter(data_type == &quot;acoustic_data&quot;) %&gt;% mutate(minute = floor_date(as.POSIXct(begin_clock_time, format = &quot;%H:%M:%S&quot;), &quot;minute&quot;)) %&gt;% group_by(site_id, site_name, year, visit_number, scientific_name, common_name, minute) %&gt;% summarise(detections_per_minute = n(), .groups = &#39;drop&#39;) # we work with the unfiltered data first to extract a detections per 30s column for each species per visit # Alternative method using a custom 30-second flooring function detections_per_30s &lt;- unfiltered_data %&gt;% filter(data_type == &quot;acoustic_data&quot;) %&gt;% mutate( # Convert to POSIXct time_posix = as.POSIXct(begin_clock_time, format = &quot;%H:%M:%S&quot;), # Floor to the nearest 30-second interval bin_30s = floor_date(time_posix, &quot;30 seconds&quot;) ) %&gt;% group_by(site_id, site_name, year, visit_number, scientific_name, common_name, bin_30s) %&gt;% summarise(detections_per_30s = n(), .groups = &#39;drop&#39;) # let&#39;s add the detections per minute or rate of detections for a species per visit back to the filtered data data &lt;- left_join(data, detections_per_30s) %&gt;% replace_na(list(abundance = 0, detections_per_30s = 0)) # visualize calling rate across species fig_callingRate &lt;- data %&gt;% filter(common_name != &quot;Scarlet Tanager&quot;) %&gt;% # since it was excluded from the GLMM ggplot(., aes(x = common_name, y = detections_per_30s)) + geom_boxplot(fill = &quot;#D29C44FF&quot;) + #facet_wrap(~site_name, scales =&quot;free&quot;) + theme_bw(base_family = &quot;Century Gothic&quot;) + theme( plot.title = element_text(size = 18, face = &quot;bold&quot;), axis.title = element_text(size = 16, face = &quot;bold&quot;), axis.text.x = element_text( size = 12, angle = 45, hjust = 1, face = &quot;italic&quot; ), axis.text.y = element_text(size = 14), strip.text = element_text(size = 12, face = &quot;bold&quot;) ) + labs( x = &quot;Common Name&quot;, y = &quot;Calling rate (total detections per minute per visit)&quot;, title = &quot;Calling Rate by Species&quot; ) # Hermit thrush has the highest mean rate of calling per minute across the list of 21 species ggsave(fig_callingRate, filename = &quot;figs/callingRates-by-species.png&quot;, width = 12, height = 9, device = png(), units = &quot;in&quot;, dpi = 300) dev.off() # Load required libraries library(dplyr) library(ggplot2) library(tidyr) library(lubridate) library(patchwork) library(viridis) # Function to calculate detection rates for different temporal bins calculate_detection_rates &lt;- function(data, bin_durations_sec) { results &lt;- data.frame() for(bin_sec in bin_durations_sec) { # Convert bin duration to appropriate label if(bin_sec &lt; 60) { bin_label &lt;- paste0(bin_sec, &quot;s&quot;) } else if(bin_sec &lt; 3600) { bin_label &lt;- paste0(bin_sec/60, &quot;min&quot;) } else { bin_label &lt;- paste0(bin_sec/3600, &quot;hr&quot;) } # Create time bins based on the current bin duration data_binned &lt;- data %&gt;% filter(data_type == &quot;acoustic_data&quot;) %&gt;% mutate( # Convert time to seconds from start of day for easier binning time_seconds = as.numeric(as.POSIXct(begin_clock_time, format = &quot;%H:%M:%S&quot;) - as.POSIXct(&quot;00:00:00&quot;, format = &quot;%H:%M:%S&quot;)), # Create time bins time_bin = floor(time_seconds / bin_sec), bin_duration_sec = bin_sec, bin_label = bin_label ) %&gt;% group_by(site_id, site_name, year, visit_number, scientific_name, common_name, time_bin, bin_duration_sec, bin_label) %&gt;% summarise( detections_per_bin = n(), bin_start_time = min(time_seconds), .groups = &#39;drop&#39; ) %&gt;% mutate( # Convert to detections per minute for comparison detections_per_minute = detections_per_bin / (bin_duration_sec / 60) ) results &lt;- rbind(results, data_binned) } return(results) } # Function to calculate summary statistics for sensitivity analysis calculate_sensitivity_stats &lt;- function(binned_data) { # Calculate statistics for each species and bin duration summary_stats &lt;- binned_data %&gt;% group_by(scientific_name, common_name, bin_duration_sec, bin_label) %&gt;% summarise( mean_rate = mean(detections_per_minute, na.rm = TRUE), median_rate = median(detections_per_minute, na.rm = TRUE), sd_rate = sd(detections_per_minute, na.rm = TRUE), cv_rate = sd(detections_per_minute, na.rm = TRUE) / mean(detections_per_minute, na.rm = TRUE), q25_rate = quantile(detections_per_minute, 0.25, na.rm = TRUE), q75_rate = quantile(detections_per_minute, 0.75, na.rm = TRUE), iqr_rate = IQR(detections_per_minute, na.rm = TRUE), n_bins = n(), n_zero_bins = sum(detections_per_bin == 0), prop_zero_bins = n_zero_bins / n_bins, .groups = &#39;drop&#39; ) %&gt;% # Remove infinite CV values mutate(cv_rate = ifelse(is.infinite(cv_rate) | is.nan(cv_rate), NA, cv_rate)) return(summary_stats) } # Define temporal bin durations to test (in seconds) bin_durations &lt;- c(30, 60, 120, 240, 300, 600) # 30s to 20min bin_labels &lt;- c(&quot;30s&quot;, &quot;1min&quot;, &quot;2min&quot;, &quot;4min&quot;, &quot;5min&quot;, &quot;10min&quot;) # Calculate detection rates for all bin durations cat(&quot;Calculating detection rates for different temporal bins...\\n&quot;) unfiltered_data &lt;- unfiltered_data %&gt;% semi_join(data, by = &quot;common_name&quot;) binned_detection_data &lt;- calculate_detection_rates(unfiltered_data, bin_durations) # Calculate summary statistics summary_stats &lt;- calculate_sensitivity_stats(binned_detection_data) # Filter out Scarlet Tanager to match your analysis summary_stats_filtered &lt;- summary_stats %&gt;% filter(common_name != &quot;Scarlet Tanager&quot;) # 1. Heatmap showing coefficient of variation (precision) across species and bin durations p1_cv &lt;- summary_stats_filtered %&gt;% ggplot(aes(x = factor(bin_duration_sec, levels = bin_durations, labels = bin_labels), y = reorder(common_name, mean_rate))) + geom_tile(aes(fill = cv_rate), color = &quot;white&quot;, size = 0.1) + scale_fill_viridis_c(name = &quot;Coefficient of\\nVariation&quot;, option = &quot;plasma&quot;, na.value = &quot;grey90&quot;, trans = &quot;log10&quot;) + labs( title = &quot;Precision Analysis: Coefficient of Variation by Temporal Bin Duration&quot;, subtitle = &quot;Lower values (darker) indicate more consistent detection rate estimates&quot;, x = &quot;Temporal Bin Duration&quot;, y = &quot;Species (ordered by mean calling rate)&quot; ) + theme_minimal(base_family = &quot;Century Gothic&quot;) + theme( axis.text.x = element_text(angle = 45, hjust = 1), axis.text.y = element_text(size = 8, face = &quot;italic&quot;), panel.grid = element_blank(), plot.title = element_text(size = 14, face = &quot;bold&quot;), plot.subtitle = element_text(size = 12) ) # 2. Heatmap showing proportion of zero detection bins p2_zeros &lt;- summary_stats_filtered %&gt;% ggplot(aes(x = factor(bin_duration_sec, levels = bin_durations, labels = bin_labels), y = reorder(common_name, mean_rate))) + geom_tile(aes(fill = prop_zero_bins), color = &quot;white&quot;, size = 0.1) + scale_fill_viridis_c(name = &quot;Proportion\\nZero Bins&quot;, option = &quot;viridis&quot;, direction = -1) + labs( title = &quot;Data Sparsity Analysis: Proportion of Zero-Detection Bins&quot;, subtitle = &quot;Darker colors indicate fewer zero-detection bins (better data density)&quot;, x = &quot;Temporal Bin Duration&quot;, y = &quot;Species (ordered by mean calling rate)&quot; ) + theme_minimal(base_family = &quot;Century Gothic&quot;) + theme( axis.text.x = element_text(angle = 45, hjust = 1), axis.text.y = element_text(size = 8, face = &quot;italic&quot;), panel.grid = element_blank(), plot.title = element_text(size = 14, face = &quot;bold&quot;), plot.subtitle = element_text(size = 12) ) # 3. Line plots showing how detection rates change with bin duration for each species p3_rates &lt;- summary_stats_filtered %&gt;% ggplot(aes(x = bin_duration_sec, y = mean_rate, group = common_name)) + geom_line(alpha = 0.7, color = &quot;#D29C44FF&quot;) + geom_point(alpha = 0.8, color = &quot;#D29C44FF&quot;) + facet_wrap(~common_name, scales = &quot;free_y&quot;, ncol = 4) + scale_x_continuous( breaks = bin_durations[c(1,2,3,4,5,6)], labels = bin_labels[c(1,2,3,4,5,6)] ) + labs( title = &quot;Mean Detection Rate by Temporal Bin Duration&quot;, subtitle = &quot;Individual species responses to different temporal resolutions&quot;, x = &quot;Temporal Bin Duration&quot;, y = &quot;Mean Detection Rate\\n(detections per minute)&quot; ) + theme_minimal(base_family = &quot;Century Gothic&quot;) + theme( strip.text = element_text(size = 8, face = &quot;italic&quot;), axis.text.x = element_text(angle = 45, hjust = 1, size = 7), axis.text.y = element_text(size = 7), plot.title = element_text(size = 14, face = &quot;bold&quot;), plot.subtitle = element_text(size = 12) ) # 4. Summary metrics across all species overall_summary &lt;- summary_stats_filtered %&gt;% group_by(bin_duration_sec, bin_label) %&gt;% summarise( mean_cv = mean(cv_rate, na.rm = TRUE), median_cv = median(cv_rate, na.rm = TRUE), mean_prop_zeros = mean(prop_zero_bins, na.rm = TRUE), mean_n_bins = mean(n_bins, na.rm = TRUE), n_species = n(), .groups = &#39;drop&#39; ) p4_summary &lt;- overall_summary %&gt;% pivot_longer(cols = c(mean_cv, mean_prop_zeros), names_to = &quot;metric&quot;, values_to = &quot;value&quot;) %&gt;% mutate( metric = case_when( metric == &quot;mean_cv&quot; ~ &quot;Mean Coefficient of Variation&quot;, metric == &quot;mean_prop_zeros&quot; ~ &quot;Mean Proportion Zero Bins&quot; ) ) %&gt;% ggplot(aes(x = factor(bin_duration_sec, levels = bin_durations, labels = bin_labels), y = value, group = metric, color = metric)) + geom_line(size = 1.2) + geom_point(size = 2) + facet_wrap(~metric, scales = &quot;free_y&quot;) + scale_color_manual(values = c(&quot;#D29C44FF&quot;, &quot;#2E8B57&quot;)) + labs( title = &quot;Overall Performance Metrics by Temporal Bin Duration&quot;, subtitle = &quot;Lower values generally indicate better performance&quot;, x = &quot;Temporal Bin Duration&quot;, y = &quot;Metric Value&quot; ) + theme_minimal(base_family = &quot;Century Gothic&quot;) + theme( legend.position = &quot;none&quot;, axis.text.x = element_text(angle = 45, hjust = 1), plot.title = element_text(size = 14, face = &quot;bold&quot;), plot.subtitle = element_text(size = 12) ) # Display all plots print(p1_cv) print(p2_zeros) print(p3_rates) print(p4_summary) # Print recommendations cat(&quot;\\n=== SENSITIVITY ANALYSIS RESULTS ===\\n&quot;) cat(&quot;Temporal bin durations tested:&quot;, paste(bin_labels, collapse = &quot;, &quot;), &quot;\\n\\n&quot;) # Find optimal durations based on different criteria optimal_precision &lt;- overall_summary %&gt;% filter(!is.na(mean_cv)) %&gt;% slice_min(mean_cv, n = 1) optimal_sparsity &lt;- overall_summary %&gt;% slice_min(mean_prop_zeros, n = 1) cat(&quot;Best precision (lowest CV):&quot;, optimal_precision$bin_label, &quot;\\n&quot;) cat(&quot;Best data density (fewest zero bins):&quot;, optimal_sparsity$bin_label, &quot;\\n&quot;) # Balanced recommendation balanced_score &lt;- overall_summary %&gt;% mutate( norm_cv = scale(mean_cv)[,1], norm_zeros = scale(mean_prop_zeros)[,1], combined_score = norm_cv + norm_zeros ) %&gt;% slice_min(combined_score, n = 1) cat(&quot;Recommended duration (balanced performance):&quot;, balanced_score$bin_label, &quot;\\n&quot;) cat(&quot;\\nYour current choice of 1 minute ranks #&quot;, which(overall_summary$bin_duration_sec == 60), &quot;out of&quot;, nrow(overall_summary), &quot;for balanced performance.\\n&quot;) # Show the summary table print(overall_summary) 6.4 Does calling rate vary as a function of number of individuals Here, we expect that species-specific calling rate would increase with higher counts of individuals at a site. Alternately, there is no increase in the number of individuals and the same individual might produce no variation in calling rates per minute and we may see no relationship between the two. # visualize detections per minute vs abundance by species fig_det_rate_abundance &lt;- data %&gt;% filter(common_name != &quot;Scarlet Tanager&quot;) %&gt;% # since it was excluded from the GLMM ggplot(., aes(x = abundance, y = detections_per_minute)) + geom_point(color = &quot;#D29C44FF&quot;, alpha = 0.7, size = 2) + geom_smooth(method = &quot;lm&quot;, se = TRUE, color = &quot;black&quot;, linewidth = 1) + facet_wrap(~common_name, scales = &quot;free&quot;) + theme_bw(base_family = &quot;Century Gothic&quot;) + theme( plot.title = element_text(size = 18, face = &quot;bold&quot;), axis.title = element_text(size = 16, face = &quot;bold&quot;), axis.text.x = element_text(size = 12), axis.text.y = element_text(size = 12), strip.text = element_text(size = 12, face = &quot;bold&quot;) ) + labs( x = &quot;Abundance&quot;, y = &quot;Calling rate&quot;, title = &quot;Calling rate vs Abundance by Species&quot; ) # several species show an increase in rate of detections per minute with increasing number of individuals ggsave(fig_det_rate_abundance, filename = &quot;figs/calling-rate-vs-abundance-by-species.png&quot;, width = 14, height = 10, device = png(), units = &quot;in&quot;, dpi = 300) dev.off() 6.5 Is calling rate linked to the fit between predicted abundance and acoustic detections? Here, we ask if calling rate governs the strength of the relationship between predicted abundance and acoustic detections as estimated from the generalized linear mixed effect model (previous script)? # extract the mean, median, and max calling rate per species calling_rate_summary &lt;- data %&gt;% filter(common_name != &quot;Scarlet Tanager&quot;) %&gt;% group_by(common_name) %&gt;% summarise( mean_detections_per_minute = mean(detections_per_minute, na.rm = TRUE), median_detections_per_minute = median(detections_per_minute, na.rm = TRUE), max_detections_per_minute = max(detections_per_minute, na.rm = TRUE), n_observations = n(), .groups = &#39;drop&#39; ) # join the psuedo r2 values and significance of fit to a single dataframe for_plot &lt;- left_join(summary_stats[,c(1,11,12)],calling_rate_summary, by = c(&quot;Species&quot; = &quot;common_name&quot;)) # note: we remove species that have a ns or no significant fit/association between predicted abundance and acoustic detections # plot mean detections fig_pseudoR2_mean_detections &lt;- for_plot %&gt;% filter(Significance != &quot;ns&quot;) %&gt;% ggplot(., aes(x = mean_detections_per_minute, y = Pseudo_R2)) + geom_point(color = &quot;#D29C44FF&quot;, size = 3, alpha = 0.8) + geom_smooth(method = &quot;lm&quot;, se = TRUE, color = &quot;black&quot;, linewidth = 1) + geom_text_repel(aes(label = Species), size = 3, fontface = &quot;italic&quot;, max.overlaps = 15) + theme_bw(base_family = &quot;Century Gothic&quot;) + theme( plot.title = element_text(size = 18, face = &quot;bold&quot;), axis.title = element_text(size = 16, face = &quot;bold&quot;), axis.text = element_text(size = 12) ) + labs( x = &quot;Mean Calling Rate (mean detections per minute)&quot;, y = &quot;Pseudo R²&quot;, title = &quot;Pseudo R² vs Mean Calling Rate Across Species&quot; ) # negative association between pseudo R2 and mean detections per minute ggsave(fig_pseudoR2_mean_detections, filename = &quot;figs/pseudoR2-vs-mean-calling-rate.png&quot;, width = 12, height = 7, device = png(), units = &quot;in&quot;, dpi = 300) dev.off() # max detections per minute fig_pseudoR2_max_detections &lt;- for_plot %&gt;% filter(Significance != &quot;ns&quot;) %&gt;% ggplot(., aes(x = max_detections_per_minute, y = Pseudo_R2)) + geom_point(color = &quot;#D29C44FF&quot;, size = 3, alpha = 0.8) + geom_smooth(method = &quot;lm&quot;, se = TRUE, color = &quot;black&quot;, linewidth = 1) + geom_text_repel(aes(label = Species), size = 3, fontface = &quot;italic&quot;, max.overlaps = 15) + theme_bw(base_family = &quot;Century Gothic&quot;) + theme( plot.title = element_text(size = 18, face = &quot;bold&quot;), axis.title = element_text(size = 16, face = &quot;bold&quot;), axis.text = element_text(size = 12) ) + labs( x = &quot;Max Calling Rate (max detections per minute)&quot;, y = &quot;Pseudo R²&quot;, title = &quot;Pseudo R² vs Max Calling Rate Across Species&quot; ) # negative association between pseudo R2 and max detections per minute ggsave(fig_pseudoR2_max_detections, filename = &quot;figs/pseudoR2-vs-max-calling-rate.png&quot;, width = 12, height = 7, device = png(), units = &quot;in&quot;, dpi = 300) dev.off() -do it per hour as well? -then plot it against the number of individuals - "],["hierarchical-model-approach.html", "Section 7 Hierarchical model approach 7.1 Packages 7.2 Loading and exploring data 7.3 Hierarchical model for counts with two observation processes", " Section 7 Hierarchical model approach Here we want to evaluate the use of another approach for a hierarchical model of estimation of counts. First, we fit a dynamic autoregressive model to estimate stochastic counts that can be compared to conventional approaches (e.g., -mixture models). We can use this autoregressive model also to estimate the same latent counts from the acoustic detections data as observation process, including an extra parameter of scaling the acoustic activity (e.g., one individual singing more than a 1:1 relationship). We try this option first with a single species in a single site. However, this can be further extrapolated to use multispecies models, from abundant species in the community to estimate parameters of rare ones (sharing some characteristics), such as Beta N-Mixture and Normal N-Mixture . 7.1 Packages library(tidyverse) library(MetBrewer) library(dclone) library(mcmcplots) 7.2 Loading and exploring data Let’s load the file used in 01_data-comparability.Rmd script. acous_count &lt;- read_csv(&quot;results/pooled_pointCount_acoustic_data.csv&quot;) acous_count |&gt; # names() group_by(common_name, data_type, site_name) |&gt; count() #|&gt; View() # use filter in RStudio Let’s work on the species-site combination of more data: Ovenbird in Marsh-Billings-Rockefeller NHP. acous_count_filt &lt;- acous_count |&gt; filter(common_name %in% c( #&quot;American Crow&quot;,&quot;American Robin&quot;,&quot;Black-and-white Warbler&quot;,&quot;Black-capped Chickadee&quot;,&quot;Black-throated Blue Warbler&quot;,&quot;Black-throated Green Warbler&quot;,&quot;Blackburnian Warbler&quot;,&quot;Blue Jay&quot;,&quot;Blue-headed Vireo&quot;,&quot;Brown Creeper&quot;,&quot;Eastern Wood-Pewee&quot;,&quot;Golden-crowned Kinglet&quot;,&quot;Hermit Thrush&quot;,&quot;Northern Parula&quot;, &quot;Ovenbird&quot; #,&quot;Pine Warbler&quot;,&quot;Red-breasted Nuthatch&quot;,&quot;Red-eyed Vireo&quot;,&quot;Scarlet Tanager&quot;,&quot;Winter Wren&quot;,&quot;Yellow-bellied Sapsucker&quot;,&quot;Yellow-rumped Warbler&quot; ), site_name == &quot;Marsh-Billings-Rockefeller NHP&quot;) table(acous_count_filt$common_name, acous_count_filt$data_type) The package naniar allow to see a good summar of NAs per column naniar::miss_var_summary(acous_count_filt) How to reconcile time of detection of each record? While the point_count data include the observation_time variable, this information for the acoustic_data seems to be at the begin_clock_time variable. Let’s adjust this with the function coalesce from dplyr in tidyverse. Then, we can consolidate the “initial temporal window” per visit in each site_id with the detection time. acoust_count_summary &lt;- acous_count_filt |&gt; # dummy column to unify time of detection either by count or mutate(detection_time = coalesce(observation_time, begin_clock_time)) |&gt; # initial temporal window group_by(site_id, date, visit_number) |&gt; mutate(time_window = min(hms::as_hms(detection_time), na.rm = TRUE), time_window = hms::as_hms(time_window)) |&gt; # First summarization to extract sum of counts per point count and number of detections group_by(site_name, site_id, date, visit_number, common_name, data_type, time_window, detection_time) |&gt; summarise(abundance = sum(abundance, na.rm = TRUE), acous_detections = n()) |&gt; # Second summarization to count number of detections and extract overall counts group_by(site_name, site_id, date, visit_number, common_name, time_window) |&gt; summarise(counts = sum(abundance, na.rm = TRUE), acous_detections = sum(acous_detections)) |&gt; as.data.frame() acoust_count_summary$DateTime &lt;- as.POSIXct(paste(acoust_count_summary$date, acoust_count_summary$time_window), format = &quot;%Y-%m-%d %H:%M:%S&quot;) acoust_count_summary$Time_window &lt;- floor_date(acoust_count_summary$DateTime, &quot;10 mins&quot;) str(acoust_count_summary) And we can generate the figures. This are similar to my previous attempt, but including some days or point counts discarded by Vijay due to species richness. 7.2.1 Figures of the data Time series of the counts ggplot(acoust_count_summary, aes(x = Time_window, y = counts, color = site_name)) + geom_segment(aes(y = 0, yend = counts), alpha = 0.3) + geom_point(alpha = 0.6) + facet_wrap(~common_name, ncol = 3) + labs(x = &quot;Time&quot;, y = &quot;Number of individuals counted in a 10-minute point count&quot;, color = &quot;Site&quot;)+ scale_color_manual(values=met.brewer(&quot;Homer2&quot;, 4))+ theme_minimal()+ theme(legend.position = &quot;bottom&quot;) #ggsave(filename = &quot;figs/counts-by-time-continuous.jpg&quot;, width = 10, height = 10, units = &quot;in&quot;, dpi = 300) Counts during the morning for 22 species Time series of acoustic detections ggplot(acoust_count_summary, aes(x = Time_window, y = acous_detections, color = site_name)) + geom_segment(aes(y = 0, yend = acous_detections), alpha = 0.3) + geom_point(alpha = 0.6) + facet_wrap(~common_name, ncol = 3, scales = &quot;free_y&quot;) + labs(x = &quot;Time&quot;, y = &quot;Number of acoustic detections in a 10-minute recording&quot;, color = &quot;Site&quot;)+ scale_color_manual(values=met.brewer(&quot;Homer2&quot;, 4))+ theme_minimal()+ theme(legend.position = &quot;bottom&quot;) #ggsave(filename = &quot;figs/acou_det-by-time-continuous.jpg&quot;, width = 10, height = 10, units = &quot;in&quot;, dpi = 300) Acoustic detections during the morning for 22 species This summary dataset also allow to see the potential effect of phenology. We can see the two years independently acoust_count_summary |&gt; mutate(year = year(date)) |&gt; filter(year == 2022) |&gt; ggplot(aes(x = Time_window, y = counts, color = site_name))+ geom_segment(aes(y = 0, yend = counts), alpha = 0.3) + geom_point(alpha = 0.6) + facet_wrap(~common_name, ncol = 3) + labs(x = &quot;Date-Time of survey (2022)&quot;, y = &quot;Number of individuals counted in a 10-minute point count&quot;, color = &quot;Site&quot;)+ scale_color_manual(values=met.brewer(&quot;Homer2&quot;, 4))+ theme_minimal()+ theme(legend.position = &quot;bottom&quot;) #ggsave(filename = &quot;figs/counts-by-date_2022.jpg&quot;, width = 10, height = 10, units = &quot;in&quot;, dpi = 300) Counts 2022 - phenology for 22 species acoust_count_summary |&gt; mutate(year = year(date)) |&gt; filter(year == 2022) |&gt; ggplot(aes(x = Time_window, y = acous_detections, color = site_name))+ geom_segment(aes(y = 0, yend = acous_detections), alpha = 0.3) + geom_point(alpha = 0.6) + facet_wrap(~common_name, ncol = 3, scales = &quot;free_y&quot;) + labs(x = &quot;Date-Time of survey (2022)&quot;, y = &quot;Number of acoustic detections in a 10-minute recording&quot;, color = &quot;Site&quot;)+ scale_color_manual(values=met.brewer(&quot;Homer2&quot;, 4))+ theme_minimal()+ theme(legend.position = &quot;bottom&quot;) #ggsave(filename = &quot;figs/acou_det-by-date_2022.jpg&quot;, width = 10, height = 10, units = &quot;in&quot;, dpi = 300) Acoustic detections 2022 - phenology for 22 species And for 2023: acoust_count_summary |&gt; mutate(year = year(date)) |&gt; filter(year == 2023) |&gt; ggplot(aes(x = DateTime, y = counts, color = site_name))+ geom_segment(aes(y = 0, yend = counts), alpha = 0.3) + geom_point(alpha = 0.6) + facet_wrap(~common_name, ncol = 3) + labs(x = &quot;Date of survey (2023)&quot;, y = &quot;Number of individuals counted in a 10-minute point count&quot;, color = &quot;Site&quot;)+ scale_color_manual(values=met.brewer(&quot;Homer2&quot;, 4))+ theme_minimal()+ theme(legend.position = &quot;bottom&quot;) #ggsave(filename = &quot;figs/counts-by-date_2023.jpg&quot;, width = 10, height = 10, units = &quot;in&quot;, dpi = 300) Counts 2023 - phenology for 22 species acoust_count_summary |&gt; mutate(year = year(date)) |&gt; filter(year == 2023) |&gt; ggplot(aes(x = DateTime, y = acous_detections, color = site_name))+ geom_segment(aes(y = 0, yend = acous_detections), alpha = 0.3) + geom_point(alpha = 0.6) + facet_wrap(~common_name, ncol = 3, scales = &quot;free_y&quot;) + labs(x = &quot;Date of survey (2023)&quot;, y = &quot;Number of acoustic detections in a 10-minute recording&quot;, color = &quot;Site&quot;)+ scale_color_manual(values=met.brewer(&quot;Homer2&quot;, 4))+ theme_minimal()+ theme(legend.position = &quot;bottom&quot;) #ggsave(filename = &quot;figs/acou_det-by-date_2023.jpg&quot;, width = 10, height = 10, units = &quot;in&quot;, dpi = 300) Acoustic detections 2023 - phenology for 22 species These figures serve to see that the date of visit have a more clear effect in acoustic detections than on counts. We can incorporate these covariates in a hierarchical model. These figures also allow to estimate the variance-to-mean ratio (VMR) of our two data sources (counts and acoustic detections) to select a family distribution accordingly. Recall a simple, clear table of VMR values: VMR_table &lt;- data.frame( Distribution = c(&quot;Constant random variable&quot;, &quot;Binomial&quot;,&quot;Poisson&quot;, &quot;Negative binomial&quot;), VMR = c(&quot;VMR = 0&quot;, &quot;0&lt;VMR&lt;1&quot;, &quot;VMR = 1&quot;, &quot;VMR&gt;1&quot;) ) knitr::kable(VMR_table) So, estimating these VMR per species: acoust_count_summary |&gt; group_by(common_name) |&gt; summarise(meanCounts = mean(counts), varCounts = var(counts), VMR_counts = varCounts/meanCounts, meanAcouDet = mean(acous_detections), varAcouDet = var(acous_detections), VMR_AcouDet = varAcouDet/meanAcouDet) |&gt; #summary() as.data.frame() # to see all the table We can see that VMR_counts is between 0 and 1, while VMR_AcousticDetection &gt; 1. These ranges suggest using a Binomial and a Negative binomial distributions, respectively, in the observation process. 7.3 Hierarchical model for counts with two observation processes 7.3.1 State process for morning expected counts Assuming that every morning of sampling we find a latent number of individuals in each site (including several point counts) that fluctuates around a mean stationary number, we can see some similarities with a Stochastic Gompertz State Space population dynamics model, which in continuous time follows a Gaussian (Ornstein-Uhlenbeck, OU) diffusion dynamic. Let’s model the latent process of log-counts \\(X_{i,t} = \\ln(C_{i,t})\\) for species \\(i\\) in time \\(t\\) (sampling point count) in a site \\(m\\). We can simplify this (for now) by only working in a site (\\(m = 1\\); Marsh-Billings-Rockefeller NHP) and a single species example (\\(i = 1\\); Ovenbird - OVEN). On the logarithmic scale, the process becomes linear and follows an autoregressive model of order 1 (note we are not including the index \\(m\\) nor \\(i\\) for now): \\[ X_{t+1} = a + c * X_{t} + \\epsilon_{t}; ~ \\epsilon \\sim \\text{Normal}(0,\\sigma^2) \\] where \\(a\\) control the drift (equivalent to the initial condition at \\(t=1\\), and then the maximum growth rate in a population model), \\(c\\) controls the mean-reverting dynamic (such as the strength of density dependence, \\(c = 1 + b\\) in a population model), and \\(\\epsilon_{t}\\) captures stochasticity in the process, distributed as a normal random variable with some noise of the process \\(\\sigma^2\\). In the long-run, we can estimate the mean of the log-counts as a function of the control drift and mean-reverting dynamic: \\[ \\text{E}[X_{\\infty}] = \\frac{a}{1-c} \\] Also the variance of the log-counts is given by the ratio of variance in stochasticity and the square of the mean-reverting dynamic: \\[ \\text{Var}[X_{\\infty}] = \\frac{\\sigma^2}{1-c^2} \\] However, how to deal with Jansen’s inequality?! \\[ e^{(\\text{E}[X_\\infty])} \\leq \\text{E}[e^{(X_\\infty)}] \\] We can estimate the long-run mean count value on the original scale by adding half of the variance, which is our expected counts per species \\(i\\): \\[ \\text{E}[C_{\\infty}] = e^{(\\text{E}[X_{\\infty}] + (0.5 \\times \\text{Var}[X_{\\infty}]))} \\] 7.3.2 Observation process for morning point counts The VMR_counts revealed underdispersion, so we can us a Binomial distribution for this observation process: \\[ Y_{t}^{Counts} | C_{t} \\sim \\text{Binomial}(\\lambda_{t},~\\tau), ~ \\lambda_{t} = e^{X_{t}} = C_{t} \\] where \\(Y_{t}^{counts}\\) is the observed counts from point counts sampling, linked to the latent (unobserved) counts \\(C_{t}\\), assuming that counts are noisy realizations of the true abundance under a binomial distribution with expected “success” counts of \\(\\lambda_{t}\\) and detection probability \\(\\tau\\). 7.3.3 Observation process for morning acoustic detections The VMR_AcousticDetection revealed overdispersion, so we can use a Negative Binomial distribution for this observation process: \\[ Y_{t}^{AcoustDet}|C_t \\sim \\text{Negative Binomial}(p_{t}, r) \\] where \\(Y_{t}^{AcoustDet}\\) is the observed acoustic detections from sampling point (that overlap with point counts), \\(p_{t} = \\frac{r}{r+\\lambda_t}\\) represents the success parameter, being a ratio of the (over)dispersion parameter \\(r\\) and the sum of the overdispersion parameter and the mean expected counts \\(\\lambda_t = e^{X_t} = C_t\\). Given that the number of acoustic detection counts does not have a perfect 1:1 relationship with expected counts, we can formulate a log-linear relationship for \\(\\lambda_t\\): \\[ \\log(\\lambda_{t}) = \\alpha + \\gamma X_{t} \\] where \\(\\alpha\\) is a species-level intercept and \\(\\gamma\\) is the scaling parameter with the latent log-abundance \\(X_{t}\\). 7.3.4 Coding Observation point counts Preparing data for data cloning acoust_count_df &lt;- acoust_count_summary |&gt; mutate(Time_w_alone = format(as.POSIXct(Time_window, format = &quot;%Y-%m-%d %H:%M:%S&quot;), &quot;%H:%M&quot;), counts = counts+1, acous_detections = acous_detections+1, year = year(date), ) |&gt; filter(year == 2022) |&gt; arrange(Time_window) # generate sequence of potential point counts seq_df &lt;- acoust_count_df |&gt; distinct(date) |&gt; # keep only unique sampled dates mutate(seq = map(date, ~ seq( from = as.POSIXct(paste(.x, &quot;05:00:00&quot;)), to = as.POSIXct(paste(.x, &quot;10:00:00&quot;)), by = &quot;10 min&quot; ))) |&gt; unnest(seq) |&gt; mutate(Time_window = seq, Time_w_alone = format(seq, &quot;%H:%M&quot;)) final_df &lt;- seq_df |&gt; select(c(date, Time_w_alone)) |&gt; left_join(acoust_count_df, by = c(&quot;date&quot;, &quot;Time_w_alone&quot;)) final_df$DateTime &lt;- as.POSIXct(paste(final_df$date, final_df$Time_w_alone), format = &quot;%Y-%m-%d %H:%M&quot;) final_df$time_id &lt;- as.integer(factor(final_df$DateTime)) StochGSS.dc &lt;- function(){ # Priors on model parameters. Priors are DC1 in Lele et al (2007) a1 ~ dnorm(0,1); # constant, equivalent to the population growth rate. c1 ~ dunif(-1,1); # constant, the density dependence parameter. sig1 ~ dlnorm(-0.5,10); #variance parameter of stochastic environment (process noise) in the system stovar1 &lt;- 1/pow(sig1,2) tau1 ~ dunif(0,1); # detection probability in Binomial distribution for(k in 1:K){ # Simulate trajectory that depends on the previous mean_X1[1,k] &lt;- a1/(1-c1) # Expected value of the first realization of the process # this is drawn from the stationary distribution of the process # Equation 14 (main text) and A.4 in Appendix of Dennis et al 2006 Varno1[k] &lt;- pow(sig1,2)/(1-pow(c1,2)) #. Equation A.5 in Appendix of Dennis et al 2006 # Updating the state: Stochastic process for all time steps X1[1,k]~dnorm(mean_X1[1,k], 1/Varno1[k]); #first estimation of population #iteration of the GSS model in the data for (t in 2:qp1) { mean_X1[t,k] &lt;- a1 + c1 * X1[(t - 1),k] X1[t,k] ~ dnorm(mean_X1[t,k], stovar1) # Et is included here since a+cX(t-1) + Et ~ Normal(a+cX(t-1),sigma^2) } # Updating the observations, from the counts under Binomial observation error for (t in 1:qp1) { N1[t,k] ~ dpois(exp(X1[t,k])) # trick to convert continuous exp(X1) to integer counts as trials Y1[t,k] ~ dbin(tau1, N1[t,k]) # } } } For this model, we will need some initial values of the parameters, and given that there could be many NA a wrap to provide those initial guess values. guess.calc &lt;- function(Yobs,Tvec){ T.t &lt;-Tvec-Tvec[1]; # For calculations, time starts at zero. q &lt;- length(Yobs)-1; # Number of time series transitions, q. qp1 &lt;- q+1; # q+1 gets used a lot, too. S.t &lt;- T.t[2:qp1]-T.t[1:q]; # Time intervals. Ybar &lt;- mean(Yobs); Yvar &lt;- sum((Yobs-Ybar)*(Yobs-Ybar))/q; mu1 &lt;- Ybar; # Kludge an initial value for theta based on mean of Y(t+s) given Y(t). th1&lt;- -mean(log(abs((Yobs[2:qp1]-mu1)/(Yobs[1:q]-mu1)))/S.t); bsq1&lt;- 2*th1*Yvar/(1+2*th1); # Moment estimate using stationary tsq1&lt;- bsq1; # variance, with betasq=tausq. #three 0&#39;s three0s &lt;- sum(c(th1,bsq1,tsq1)) if(three0s==0|is.na(three0s)){th1 &lt;- 0.5;bsq1 &lt;- 0.09; tsq1 &lt;- 0.23;} out1 &lt;- c(th1,bsq1,tsq1); if(sum(out1&lt;1e-7)&gt;=1){out1 &lt;- c(0.5,0.09,0.23)} out &lt;- c(mu1,out1); return(abs(out)) } guess.calc2.0&lt;- function(TimeAndNs){ newmat &lt;- TimeAndNs isnas &lt;- sum(is.na(TimeAndNs)) if(isnas &gt;= 1){ isnaind &lt;- which(is.na(TimeAndNs[,2]), arr.ind=TRUE) newmat &lt;- TimeAndNs[-isnaind,] newmat[,1] &lt;- newmat[,1] - newmat[1,1] } init.guess &lt;- guess.calc(Yobs = log(newmat[,2]), Tvec=newmat[,1]) mu1 &lt;- init.guess[1] th1 &lt;- init.guess[2] bsq1 &lt;- init.guess[3] sigsq1&lt;- ((1-exp(-2*th1))*bsq1)/(2*th1) out &lt;- c(mu=mu1, theta=th1, sigmasq = sigsq1) return(out) } And we have to bundle the data for Data cloning. Let’s fit the first species in the taxa with higher data (Ovenbird in Marsh-Billings-Rockefeller NHP) as a test. ts.4guess &lt;- final_df$counts tvec4guess &lt;- 1:length(ts.4guess) onets4guess &lt;- cbind(tvec4guess, ts.4guess) naive.guess &lt;- guess.calc2.0(TimeAndNs = onets4guess) datalistGSS.dc &lt;- list(K = 1, qp1 = length(ts.4guess), Y1 = dcdim(array(ts.4guess, dim = c(length(ts.4guess),1)))) dcrun.GSS &lt;- dc.fit(data = datalistGSS.dc, params = c(&quot;a1&quot;, &quot;c1&quot;, &quot;sig1&quot;, &quot;tau1&quot;), model = StochGSS.dc, n.clones = c(1,5,10,20), multiply = &quot;K&quot;, unchanged = &quot;qp1&quot;, n.chains = 3, n.adapt = 5000, n.update = 100, thin = 20, n.iter = 20000) saveRDS(dcrun.GSS, &quot;data/dcfitGSS_2022_OVEN_MBR.rds&quot;) dcrun.GSS &lt;- readRDS(&quot;data/dcfitGSS_2022_OVEN_MBR.rds&quot;) summary(dcrun.GSS); dcdiag(dcrun.GSS) plot(dcdiag(dcrun.GSS)) pairs(dcrun.GSS) coef(dcrun.GSS) Data cloning results for four parameters of GSS from point counts And with these coefficients of the model, we can estimate latent count trajectories with the Kalman filter structure. This Kalman filter structure allows simultaneous estimation of latent states for observed time steps and prediction for missing ones, leveraging the temporal correlation in the process model. Kalman.pred.fn &lt;- function() { # Priors on model parameters: they are on the real line. parms ~ dmnorm(MuPost,PrecPost) a1 &lt;- parms[1] c1 &lt;- parms[2] sig1 &lt;- parms[3] stovar1 &lt;- 1/pow(sig1,2) tau1 ~ dunif(0,1) # Likelihood mean_X1[1] &lt;- a1/(1-c1) # Expected value of the first realization of the process # this is drawn from the stationary distribution of the process # Equation 14 (main text) and A.4 in Appendix of Dennis et al 2006 Varno1 &lt;- pow(sig1,2)/(1-pow(c1,2)) #. Equation A.5 in Appendix of Dennis et al 2006 # Updating the state: Stochastic process for all time steps X1[1]~dnorm(mean_X1[1], 1/Varno1); #first estimation of population C[1] &lt;- exp(X1[1]) #iteration of the GSS model in the data for (t in 2:qp1) { mean_X1[t] &lt;- a1 + c1 * X1[(t - 1)] X1[t] ~ dnorm(mean_X1[t], stovar1) # Et is included here since a+cX(t-1) + Et ~ Normal(a+cX(t-1),sigma^2) N[t] ~ dpois(exp(X1[t])) # trick to convert continuous exp(X1) to integer counts as trials in binomial Y1[(t-1)] ~ dbin(tau1, N[t]) # C[t] &lt;- exp(X1[t]) # expected count } } data4kalman &lt;- list(qp1 = as.numeric(length(final_df$counts)), Y1 = array(final_df$counts, dim = c(as.numeric(length(final_df$counts)))), MuPost = coef(dcrun.GSS), PrecPost = solve(vcov(dcrun.GSS))) And run the Bayesian inference using the MLE from Data cloning BH_DC_Pred = jags.fit(data=data4kalman, params=c(&quot;C&quot;), model=Kalman.pred.fn) And we can generate a dataframe of the time series with the estimates, inter quartile range, and observed data through time summary(BH_DC_Pred) # extract predictions and CI around them pred &lt;- as.data.frame(t(mcmcapply(BH_DC_Pred, quantile, c(0.025, 0.5, 0.975)))) ExpectedCounts &lt;- as.data.frame(cbind(final_df,pred)) # modify names names(ExpectedCounts) &lt;- c(&quot;date&quot;,&quot;Time_w_alone&quot;,&quot;site_name&quot;,&quot;site_id&quot;, &quot;visit_number&quot;,&quot;common_name&quot;,&quot;time_window&quot;, &quot;Observed&quot;, &quot;acous_detections&quot;,&quot;DateTime&quot;,&quot;Time_window&quot;, &quot;year&quot;, &quot;time_id&quot;,&quot;Lower&quot;, &quot;Estimated&quot;, &quot;Upper&quot;) ExpectedCounts |&gt; pivot_longer(cols = c(Lower, Estimated, Upper, Observed), names_to = &quot;Abundance&quot;, values_to = &quot;Count&quot;) |&gt; ggplot(aes(x = DateTime, y = Count, color = factor(Abundance, levels = c(&quot;Observed&quot;, &quot;Upper&quot;, &quot;Estimated&quot;, &quot;Lower&quot;)))) + geom_line(aes(linetype = factor(Abundance, levels = c(&quot;Observed&quot;, &quot;Upper&quot;, &quot;Estimated&quot;, &quot;Lower&quot;)))) + geom_point(aes(shape = factor(Abundance, levels = c(&quot;Observed&quot;, &quot;Upper&quot;, &quot;Estimated&quot;, &quot;Lower&quot;)))) + labs(title = &quot;OVEN - Marsh-Billings-Rockefeller NHP - 2022 sampling&quot;, x = &quot;Time (sampling window)&quot;, y = &quot;Counts&quot;, color = &quot;Counts&quot;, linetype = &quot;Counts&quot;, shape = &quot;Counts&quot;) + scale_linetype_manual(values = c(NA,&quot;dashed&quot;,&quot;solid&quot;,&quot;dashed&quot;))+ scale_shape_manual(values = c(21, NA,NA,NA)) + scale_color_manual(values = c(&quot;blue&quot;,&quot;darkgray&quot;,&quot;black&quot;,&quot;darkgray&quot;)) + theme_classic() + theme(legend.position = &quot;bottom&quot;) 7.3.5 Coding Acoustic detections counts StochGSS.acou.dc &lt;- function(){ # Priors on model parameters. Priors are DC1 in Lele et al (2007) a1 ~ dnorm(0,1); # constant, equivalent to the population growth rate. c1 ~ dunif(-1,1); # constant, the density dependence parameter. sig1 ~ dlnorm(-0.5,10); #variance parameter of stochastic environment (process noise) in the system stovar1 &lt;- 1/pow(sig1,2) # Priors for observation model alpha ~ dnorm(0, 0.001) # intercept for scaling gamma ~ dnorm(0, 0.001) # slope linking latent state X_t to mean r ~ dunif(0,50) # dispersion parameter for NegBin for(k in 1:K){ # Simulate trajectory that depends on the previous mean_X1[1,k] &lt;- a1/(1-c1) # Expected value of the first realization of the process # this is drawn from the stationary distribution of the process # Equation 14 (main text) and A.4 in Appendix of Dennis et al 2006 Varno1[k] &lt;- pow(sig1,2)/(1-pow(c1,2)) #. Equation A.5 in Appendix of Dennis et al 2006 # Updating the state: Stochastic process for all time steps X1[1,k]~dnorm(mean_X1[1,k], 1/Varno1[k]); #first estimation of population #iteration of the GSS model in the data for (t in 2:qp1) { mean_X1[t,k] &lt;- a1 + c1 * X1[(t - 1),k] X1[t,k] ~ dnorm(mean_X1[t,k], stovar1) # Et is included here since a+cX(t-1) + Et ~ Normal(a+cX(t-1),sigma^2) } # Updating the observations, from the acoustic detections under Negative Binomial observation error for (t in 1:qp1) { Y1[t,k] ~ dnegbin(p[t,k], r) p[t,k] &lt;- r/(r+lambda[t,k]) log(lambda[t,k]) &lt;- alpha + gamma * X1[t,k] } } } For this model, we will need some initial values of the parameters, and given that there could be many NA a wrap to provide those initial guess values. guess.calc &lt;- function(Yobs,Tvec){ T.t &lt;-Tvec-Tvec[1]; # For calculations, time starts at zero. q &lt;- length(Yobs)-1; # Number of time series transitions, q. qp1 &lt;- q+1; # q+1 gets used a lot, too. S.t &lt;- T.t[2:qp1]-T.t[1:q]; # Time intervals. Ybar &lt;- mean(Yobs); Yvar &lt;- sum((Yobs-Ybar)*(Yobs-Ybar))/q; mu1 &lt;- Ybar; # Kludge an initial value for theta based on mean of Y(t+s) given Y(t). th1&lt;- -mean(log(abs((Yobs[2:qp1]-mu1)/(Yobs[1:q]-mu1)))/S.t); bsq1&lt;- 2*th1*Yvar/(1+2*th1); # Moment estimate using stationary tsq1&lt;- bsq1; # variance, with betasq=tausq. #three 0&#39;s three0s &lt;- sum(c(th1,bsq1,tsq1)) if(three0s==0|is.na(three0s)){th1 &lt;- 0.5;bsq1 &lt;- 0.09; tsq1 &lt;- 0.23;} out1 &lt;- c(th1,bsq1,tsq1); if(sum(out1&lt;1e-7)&gt;=1){out1 &lt;- c(0.5,0.09,0.23)} out &lt;- c(mu1,out1); return(abs(out)) } guess.calc2.0&lt;- function(TimeAndNs){ newmat &lt;- TimeAndNs isnas &lt;- sum(is.na(TimeAndNs)) if(isnas &gt;= 1){ isnaind &lt;- which(is.na(TimeAndNs[,2]), arr.ind=TRUE) newmat &lt;- TimeAndNs[-isnaind,] newmat[,1] &lt;- newmat[,1] - newmat[1,1] } init.guess &lt;- guess.calc(Yobs = log(newmat[,2]), Tvec=newmat[,1]) mu1 &lt;- init.guess[1] th1 &lt;- init.guess[2] bsq1 &lt;- init.guess[3] sigsq1&lt;- ((1-exp(-2*th1))*bsq1)/(2*th1) out &lt;- c(mu=mu1, theta=th1, sigmasq = sigsq1) return(out) } And we have to bundle the data for Data cloning. Let’s fit the first species in the taxa with higher data (Ovenbird in Marsh-Billings-Rockefeller NHP) as a test. ts.4guess &lt;- as.numeric(final_df$acous_detections) tvec4guess &lt;- 1:length(ts.4guess) onets4guess &lt;- cbind(tvec4guess, ts.4guess) naive.guess &lt;- guess.calc2.0(TimeAndNs = onets4guess) datalistGSS.acou.dc &lt;- list(K = 1, qp1 = length(ts.4guess), Y1 = dcdim(array(ts.4guess, dim = c(length(ts.4guess),1)))) dcrun.GSS.acou &lt;- dc.fit(data = datalistGSS.acou.dc, params = c(&quot;a1&quot;, &quot;c1&quot;, &quot;sig1&quot;, &quot;gamma&quot;, &quot;alpha&quot;, &quot;r&quot;), model = StochGSS.acou.dc, n.clones = c(1,5,10,20), multiply = &quot;K&quot;, unchanged = &quot;qp1&quot;, n.chains = 3, n.adapt = 5000, n.update = 100, thin = 20, n.iter = 20000) saveRDS(dcrun.GSS.acou, &quot;data/dcfitGSS_AcDet_2022_OVEN_MBR.rds&quot;) dcrun.GSS.acou &lt;- readRDS(&quot;data/dcfitGSS_AcDet_2022_OVEN_MBR.rds&quot;) summary(dcrun.GSS.acou); dcdiag(dcrun.GSS.acou) plot(dcdiag(dcrun.GSS.acou)) pairs(dcrun.GSS.acou) coef(dcrun.GSS.acou) Data cloning results for four parameters of GSS from acoustic detections Although diagnostics for lambda.max decreases with more clones, the chains are not converging. It seems that this needs more iterations and change initial values. Anyway, with these coefficients of the model, we can estimate latent count trajectories with the Kalman filter structure. This Kalman filter structure allows simultaneous estimation of latent states for observed time steps and prediction for missing ones, leveraging the temporal correlation in the process model. Kalman.pred.acou.fn &lt;- function() { # Priors on model parameters: they are on the real line. parms ~ dmnorm(MuPost,PrecPost) a1 &lt;- parms[1] alpha &lt;- parms[2] # intercept for scaling c1 &lt;- parms[3] gamma &lt;- parms[4] # slope linking latent state X_t to mean r &lt;- parms[5] # overdispersion parameter for NegBinom sig1 &lt;- parms[6] stovar1 &lt;- 1/pow(sig1,2) # Likelihood mean_X1[1] &lt;- a1/(1-c1) # Expected value of the first realization of the process # this is drawn from the stationary distribution of the process # Equation 14 (main text) and A.4 in Appendix of Dennis et al 2006 Varno1 &lt;- pow(sig1,2)/(1-pow(c1,2)) #. Equation A.5 in Appendix of Dennis et al 2006 # Updating the state: Stochastic process for all time steps X1[1]~dnorm(mean_X1[1], 1/Varno1); #first estimation of population log(lambda[1]) &lt;- alpha + gamma * X1[1] C[1] &lt;- log(lambda[1]) #iteration of the GSS model in the data for (t in 2:qp1) { mean_X1[t] &lt;- a1 + c1 * X1[(t - 1)] X1[t] ~ dnorm(mean_X1[t], stovar1) # Et is included here since a+cX(t-1) + Et ~ Normal(a+cX(t-1),sigma^2) Y1[(t-1)] ~ dnegbin(p[t], r) p[t] &lt;- r/(r+lambda[t]) log(lambda[t]) &lt;- alpha + gamma * X1[t] C[t] &lt;- log(lambda[t]) # expected count } } data4kalmanAco &lt;- list(qp1 = as.numeric(length(final_df$acous_detections)), Y1 = array(final_df$acous_detections, dim = c(as.numeric(length(final_df$acous_detections)))), MuPost = coef(dcrun.GSS.acou), PrecPost = solve(vcov(dcrun.GSS.acou))) And run the Bayesian inference using the MLE from Data cloning BH_DC_Pred_Acou = jags.fit(data=data4kalmanAco, params=c(&quot;C&quot;), model=Kalman.pred.acou.fn) And we can generate a dataframe of the time series with the estimates, inter quartile range, and observed data through time summary(BH_DC_Pred_Acou) # extract predictions and CI around them predAcou &lt;- as.data.frame(t(mcmcapply(BH_DC_Pred_Acou, quantile, c(0.025, 0.5, 0.975)))) ExpectedCountsAcou &lt;- as.data.frame(cbind(ExpectedCounts,predAcou)) # modify names names(ExpectedCountsAcou) &lt;- c(&quot;date&quot;,&quot;Time_w_alone&quot;,&quot;site_name&quot;,&quot;site_id&quot;, &quot;visit_number&quot;,&quot;common_name&quot;,&quot;time_window&quot;, &quot;Observed&quot;, &quot;AcousticDetections&quot;, &quot;DateTime&quot;,&quot;Time_window&quot;, &quot;year&quot;, &quot;time_id&quot;, &quot;CountLower&quot;, &quot;CountEstimated&quot;, &quot;CountUpper&quot;, &quot;AcouDetLower&quot;, &quot;AcouDetEstimated&quot;, &quot;AcouDetUpper&quot;) par(mar = c(5, 4, 7, 2)) # low, left, up, right plot(x = ExpectedCountsAcou$DateTime, y = ExpectedCountsAcou$Observed, col = &quot;#1b7837&quot;, ylim = c(0,40), ylab = &quot;Counts&quot;, xlab = &quot;Time (sampling window)&quot;) points(x = ExpectedCountsAcou$DateTime, y = ExpectedCountsAcou$AcousticDetections, col = &quot;#762a83&quot;) lines(x = ExpectedCountsAcou$DateTime, y = ExpectedCountsAcou$CountEstimated, col = &quot;#7fbf7b&quot;) lines(x = ExpectedCountsAcou$DateTime, y = ExpectedCountsAcou$CountLower, col = &quot;#d9f0d3&quot;, lty = 2) lines(x = ExpectedCountsAcou$DateTime, y = ExpectedCountsAcou$CountUpper, col = &quot;#d9f0d3&quot;, lty = 2) lines(x = ExpectedCountsAcou$DateTime, y = ExpectedCountsAcou$AcouDetEstimated, col = &quot;#af8dc3&quot;) lines(x = ExpectedCountsAcou$DateTime, y = ExpectedCountsAcou$AcouDetLower, col = &quot;#e7d4e8&quot;, lty = 2) lines(x = ExpectedCountsAcou$DateTime, y = ExpectedCountsAcou$AcouDetUpper, col = &quot;#e7d4e8&quot;, lty = 2) legend(x = min(ExpectedCountsAcou$DateTime), y = 65, cex = 0.7, pt.cex = 0.7, xpd = TRUE, horiz = FALSE, legend = c(&quot;Observed from point counts&quot;, &quot;Acoustic detections&quot;, &quot;Estimate from point counts&quot;, &quot;Point count CI estimates&quot;, &quot;Estimate from acoustic detections&quot;, &quot;Acoustic detections CI estimates&quot;), col = c(&quot;#1b7837&quot;, &quot;#762a83&quot;, &quot;#7fbf7b&quot;, &quot;#d9f0d3&quot;, &quot;#af8dc3&quot;, &quot;#e7d4e8&quot;), lty = c(NA, NA, 1, 2, 1, 2), # NA for points, 1 for solid lines, 2 for dashed pch = c(1, 1, NA, NA, NA, NA), # symbols for points bty = &quot;n&quot;) # no box around legend Observed and estimated counts from point counts and acoustic detections - A Cutting the edges to see the details of the estimates: par(mar = c(6, 4, 4, 2)) plot(x = ExpectedCountsAcou$DateTime, y = ExpectedCountsAcou$Observed, col = &quot;#1b7837&quot;, ylim = c(0,10), ylab = &quot;Counts&quot;, xlab = &quot;Time (sampling window)&quot;, main = &quot;OVEN - 2022 - Marsh-Billings-Rockefeller NHP&quot;) points(x = ExpectedCountsAcou$DateTime, y = ExpectedCountsAcou$AcousticDetections, col = &quot;#762a83&quot;) lines(x = ExpectedCountsAcou$DateTime, y = ExpectedCountsAcou$CountEstimated, col = &quot;#7fbf7b&quot;) lines(x = ExpectedCountsAcou$DateTime, y = ExpectedCountsAcou$CountLower, col = &quot;#d9f0d3&quot;, lty = 2) lines(x = ExpectedCountsAcou$DateTime, y = ExpectedCountsAcou$CountUpper, col = &quot;#d9f0d3&quot;, lty = 2) lines(x = ExpectedCountsAcou$DateTime, y = ExpectedCountsAcou$AcouDetEstimated, col = &quot;#af8dc3&quot;) lines(x = ExpectedCountsAcou$DateTime, y = ExpectedCountsAcou$AcouDetLower, col = &quot;#e7d4e8&quot;, lty = 2) lines(x = ExpectedCountsAcou$DateTime, y = ExpectedCountsAcou$AcouDetUpper, col = &quot;#e7d4e8&quot;, lty = 2) legend(x = min(ExpectedCountsAcou$DateTime), y = 10, cex = 0.7, pt.cex = 0.7, xpd = TRUE, horiz = FALSE, legend = c(&quot;Observed from point counts&quot;, &quot;Acoustic detections&quot;, &quot;Estimate from point counts&quot;, &quot;Point count CI estimates&quot;, &quot;Estimate from acoustic detections&quot;, &quot;Acoustic detections CI estimates&quot;), col = c(&quot;#1b7837&quot;, &quot;#762a83&quot;, &quot;#7fbf7b&quot;, &quot;#d9f0d3&quot;, &quot;#af8dc3&quot;, &quot;#e7d4e8&quot;), lty = c(NA, NA, 1, 2, 1, 2), # NA for points, 1 for solid lines, 2 for dashed pch = c(1, 1, NA, NA, NA, NA), # symbols for points bty = &quot;n&quot;) # no box around legend Observed and estimated counts from point counts and acoustic detections - B "],["another-example-with-hierarchical-model-approach---oven-in-katahdin-woods-and-waters.html", "Section 8 Another example with hierarchical model approach - OVEN in Katahdin Woods and Waters 8.1 Packages 8.2 Loading and exploring data 8.3 Hierarchical model for counts with two observation processes", " Section 8 Another example with hierarchical model approach - OVEN in Katahdin Woods and Waters 8.1 Packages library(tidyverse) library(MetBrewer) library(dclone) library(mcmcplots) 8.2 Loading and exploring data Let’s load the file used in 01_data-comparability.Rmd script. acous_count &lt;- read_csv(&quot;results/pooled_pointCount_acoustic_data.csv&quot;) Let’s work on the species-site combination of more data: Ovenbird in Marsh-Billings-Rockefeller NHP. acous_count_filt &lt;- acous_count |&gt; filter(common_name %in% c( #&quot;American Crow&quot;,&quot;American Robin&quot;,&quot;Black-and-white Warbler&quot;,&quot;Black-capped Chickadee&quot;,&quot;Black-throated Blue Warbler&quot;,&quot;Black-throated Green Warbler&quot;,&quot;Blackburnian Warbler&quot;,&quot;Blue Jay&quot;,&quot;Blue-headed Vireo&quot;,&quot;Brown Creeper&quot;,&quot;Eastern Wood-Pewee&quot;,&quot;Golden-crowned Kinglet&quot;,&quot;Hermit Thrush&quot;,&quot;Northern Parula&quot;, &quot;Ovenbird&quot; #,&quot;Pine Warbler&quot;,&quot;Red-breasted Nuthatch&quot;,&quot;Red-eyed Vireo&quot;,&quot;Scarlet Tanager&quot;,&quot;Winter Wren&quot;,&quot;Yellow-bellied Sapsucker&quot;,&quot;Yellow-rumped Warbler&quot; ), site_name == &quot;Katahdin Woods and Waters&quot;) table(acous_count_filt$common_name, acous_count_filt$data_type) How to reconcile time of detection of each record? While the point_count data include the observation_time variable, this information for the acoustic_data seems to be at the begin_clock_time variable. Let’s adjust this with the function coalesce from dplyr in tidyverse. Then, we can consolidate the “initial temporal window” per visit in each site_id with the detection time. acoust_count_summary &lt;- acous_count_filt |&gt; # dummy column to unify time of detection either by count or mutate(detection_time = coalesce(observation_time, begin_clock_time)) |&gt; # initial temporal window group_by(site_id, date, visit_number) |&gt; mutate(time_window = min(hms::as_hms(detection_time), na.rm = TRUE), time_window = hms::as_hms(time_window)) |&gt; # First summarization to extract sum of counts per point count and number of detections group_by(site_name, site_id, date, visit_number, common_name, data_type, time_window, detection_time) |&gt; summarise(abundance = sum(abundance, na.rm = TRUE), acous_detections = n()) |&gt; # Second summarization to count number of detections and extract overall counts group_by(site_name, site_id, date, visit_number, common_name, time_window) |&gt; summarise(counts = sum(abundance, na.rm = TRUE), acous_detections = sum(acous_detections)) |&gt; as.data.frame() acoust_count_summary$DateTime &lt;- as.POSIXct(paste(acoust_count_summary$date, acoust_count_summary$time_window), format = &quot;%Y-%m-%d %H:%M:%S&quot;) acoust_count_summary$Time_window &lt;- floor_date(acoust_count_summary$DateTime, &quot;10 mins&quot;) str(acoust_count_summary) And we can generate the figures. This are similar to my previous attempt, but including some days or point counts discarded by Vijay due to species richness. 8.3 Hierarchical model for counts with two observation processes 8.3.1 Coding Observation point counts Preparing data for data cloning acoust_count_df &lt;- acoust_count_summary |&gt; mutate(Time_w_alone = format(as.POSIXct(Time_window, format = &quot;%Y-%m-%d %H:%M:%S&quot;), &quot;%H:%M&quot;), counts = counts+1, acous_detections = acous_detections+1, year = year(date), ) |&gt; filter(year == 2023) |&gt; arrange(Time_window) # generate sequence of potential point counts seq_df &lt;- acoust_count_df |&gt; distinct(date) |&gt; # keep only unique sampled dates mutate(seq = map(date, ~ seq( from = as.POSIXct(paste(.x, &quot;05:00:00&quot;)), to = as.POSIXct(paste(.x, &quot;10:00:00&quot;)), by = &quot;10 min&quot; ))) |&gt; unnest(seq) |&gt; mutate(Time_window = seq, Time_w_alone = format(seq, &quot;%H:%M&quot;)) final_df &lt;- seq_df |&gt; select(c(date, Time_w_alone)) |&gt; left_join(acoust_count_df, by = c(&quot;date&quot;, &quot;Time_w_alone&quot;)) final_df$DateTime &lt;- as.POSIXct(paste(final_df$date, final_df$Time_w_alone), format = &quot;%Y-%m-%d %H:%M&quot;) final_df$time_id &lt;- as.integer(factor(final_df$DateTime)) StochGSS.dc &lt;- function(){ # Priors on model parameters. Priors are DC1 in Lele et al (2007) a1 ~ dnorm(0,1); # constant, equivalent to the population growth rate. c1 ~ dunif(-1,1); # constant, the density dependence parameter. sig1 ~ dlnorm(-0.5,10); #variance parameter of stochastic environment (process noise) in the system stovar1 &lt;- 1/pow(sig1,2) tau1 ~ dunif(0,1); # detection probability in Binomial distribution for(k in 1:K){ # Simulate trajectory that depends on the previous mean_X1[1,k] &lt;- a1/(1-c1) # Expected value of the first realization of the process # this is drawn from the stationary distribution of the process # Equation 14 (main text) and A.4 in Appendix of Dennis et al 2006 Varno1[k] &lt;- pow(sig1,2)/(1-pow(c1,2)) #. Equation A.5 in Appendix of Dennis et al 2006 # Updating the state: Stochastic process for all time steps X1[1,k]~dnorm(mean_X1[1,k], 1/Varno1[k]); #first estimation of population #iteration of the GSS model in the data for (t in 2:qp1) { mean_X1[t,k] &lt;- a1 + c1 * X1[(t - 1),k] X1[t,k] ~ dnorm(mean_X1[t,k], stovar1) # Et is included here since a+cX(t-1) + Et ~ Normal(a+cX(t-1),sigma^2) } # Updating the observations, from the counts under Binomial observation error for (t in 1:qp1) { N1[t,k] ~ dpois(exp(X1[t,k])) # trick to convert continuous exp(X1) to integer counts as trials Y1[t,k] ~ dbin(tau1, N1[t,k]) # } } } For this model, we will need some initial values of the parameters, and given that there could be many NA a wrap to provide those initial guess values. guess.calc &lt;- function(Yobs,Tvec){ T.t &lt;-Tvec-Tvec[1]; # For calculations, time starts at zero. q &lt;- length(Yobs)-1; # Number of time series transitions, q. qp1 &lt;- q+1; # q+1 gets used a lot, too. S.t &lt;- T.t[2:qp1]-T.t[1:q]; # Time intervals. Ybar &lt;- mean(Yobs); Yvar &lt;- sum((Yobs-Ybar)*(Yobs-Ybar))/q; mu1 &lt;- Ybar; # Kludge an initial value for theta based on mean of Y(t+s) given Y(t). th1&lt;- -mean(log(abs((Yobs[2:qp1]-mu1)/(Yobs[1:q]-mu1)))/S.t); bsq1&lt;- 2*th1*Yvar/(1+2*th1); # Moment estimate using stationary tsq1&lt;- bsq1; # variance, with betasq=tausq. #three 0&#39;s three0s &lt;- sum(c(th1,bsq1,tsq1)) if(three0s==0|is.na(three0s)){th1 &lt;- 0.5;bsq1 &lt;- 0.09; tsq1 &lt;- 0.23;} out1 &lt;- c(th1,bsq1,tsq1); if(sum(out1&lt;1e-7)&gt;=1){out1 &lt;- c(0.5,0.09,0.23)} out &lt;- c(mu1,out1); return(abs(out)) } guess.calc2.0&lt;- function(TimeAndNs){ newmat &lt;- TimeAndNs isnas &lt;- sum(is.na(TimeAndNs)) if(isnas &gt;= 1){ isnaind &lt;- which(is.na(TimeAndNs[,2]), arr.ind=TRUE) newmat &lt;- TimeAndNs[-isnaind,] newmat[,1] &lt;- newmat[,1] - newmat[1,1] } init.guess &lt;- guess.calc(Yobs = log(newmat[,2]), Tvec=newmat[,1]) mu1 &lt;- init.guess[1] th1 &lt;- init.guess[2] bsq1 &lt;- init.guess[3] sigsq1&lt;- ((1-exp(-2*th1))*bsq1)/(2*th1) out &lt;- c(mu=mu1, theta=th1, sigmasq = sigsq1) return(out) } And we have to bundle the data for Data cloning. Let’s fit the first species in the taxa with higher data (Ovenbird in Marsh-Billings-Rockefeller NHP) as a test. ts.4guess &lt;- final_df$counts tvec4guess &lt;- 1:length(ts.4guess) onets4guess &lt;- cbind(tvec4guess, ts.4guess) naive.guess &lt;- guess.calc2.0(TimeAndNs = onets4guess) datalistGSS.dc &lt;- list(K = 1, qp1 = length(ts.4guess), Y1 = dcdim(array(ts.4guess, dim = c(length(ts.4guess),1)))) dcrun.GSS &lt;- dc.fit(data = datalistGSS.dc, params = c(&quot;a1&quot;, &quot;c1&quot;, &quot;sig1&quot;, &quot;tau1&quot;), model = StochGSS.dc, n.clones = c(1,5,10,20), multiply = &quot;K&quot;, unchanged = &quot;qp1&quot;, n.chains = 3, n.adapt = 5000, n.update = 100, thin = 20, n.iter = 20000) saveRDS(dcrun.GSS, &quot;data/dcfitGSS_2023_OVEN_KWW.rds&quot;) dcrun.GSS &lt;- readRDS(&quot;data/dcfitGSS_2023_OVEN_KWW.rds&quot;) summary(dcrun.GSS); dcdiag(dcrun.GSS) plot(dcdiag(dcrun.GSS)) pairs(dcrun.GSS) coef(dcrun.GSS) Data cloning results for four parameters of GSS from point counts And with these coefficients of the model, we can estimate latent count trajectories with the Kalman filter structure. This Kalman filter structure allows simultaneous estimation of latent states for observed time steps and prediction for missing ones, leveraging the temporal correlation in the process model. Kalman.pred.fn &lt;- function() { # Priors on model parameters: they are on the real line. parms ~ dmnorm(MuPost,PrecPost) a1 &lt;- parms[1] c1 &lt;- parms[2] sig1 &lt;- parms[3] stovar1 &lt;- 1/pow(sig1,2) tau1 ~ dunif(0,1) # Likelihood mean_X1[1] &lt;- a1/(1-c1) # Expected value of the first realization of the process # this is drawn from the stationary distribution of the process # Equation 14 (main text) and A.4 in Appendix of Dennis et al 2006 Varno1 &lt;- pow(sig1,2)/(1-pow(c1,2)) #. Equation A.5 in Appendix of Dennis et al 2006 # Updating the state: Stochastic process for all time steps X1[1]~dnorm(mean_X1[1], 1/Varno1); #first estimation of population C[1] &lt;- exp(X1[1]) #iteration of the GSS model in the data for (t in 2:qp1) { mean_X1[t] &lt;- a1 + c1 * X1[(t - 1)] X1[t] ~ dnorm(mean_X1[t], stovar1) # Et is included here since a+cX(t-1) + Et ~ Normal(a+cX(t-1),sigma^2) N[t] ~ dpois(exp(X1[t])) # trick to convert continuous exp(X1) to integer counts as trials in binomial Y1[(t-1)] ~ dbin(tau1, N[t]) # C[t] &lt;- exp(X1[t]) # expected count } } data4kalman &lt;- list(qp1 = as.numeric(length(final_df$counts)), Y1 = array(final_df$counts, dim = c(as.numeric(length(final_df$counts)))), MuPost = coef(dcrun.GSS), PrecPost = solve(vcov(dcrun.GSS))) And run the Bayesian inference using the MLE from Data cloning BH_DC_Pred = jags.fit(data=data4kalman, params=c(&quot;C&quot;), model=Kalman.pred.fn) And we can generate a dataframe of the time series with the estimates, inter quartile range, and observed data through time summary(BH_DC_Pred) # extract predictions and CI around them pred &lt;- as.data.frame(t(mcmcapply(BH_DC_Pred, quantile, c(0.025, 0.5, 0.975)))) ExpectedCounts &lt;- as.data.frame(cbind(final_df,pred)) # modify names names(ExpectedCounts) &lt;- c(&quot;date&quot;,&quot;Time_w_alone&quot;,&quot;site_name&quot;,&quot;site_id&quot;, &quot;visit_number&quot;,&quot;common_name&quot;,&quot;time_window&quot;, &quot;Observed&quot;, &quot;acous_detections&quot;,&quot;DateTime&quot;,&quot;Time_window&quot;, &quot;year&quot;, &quot;time_id&quot;,&quot;Lower&quot;, &quot;Estimated&quot;, &quot;Upper&quot;) ExpectedCounts |&gt; pivot_longer(cols = c(Lower, Estimated, Upper, Observed), names_to = &quot;Abundance&quot;, values_to = &quot;Count&quot;) |&gt; ggplot(aes(x = DateTime, y = Count, color = factor(Abundance, levels = c(&quot;Observed&quot;, &quot;Upper&quot;, &quot;Estimated&quot;, &quot;Lower&quot;)))) + geom_line(aes(linetype = factor(Abundance, levels = c(&quot;Observed&quot;, &quot;Upper&quot;, &quot;Estimated&quot;, &quot;Lower&quot;)))) + geom_point(aes(shape = factor(Abundance, levels = c(&quot;Observed&quot;, &quot;Upper&quot;, &quot;Estimated&quot;, &quot;Lower&quot;)))) + scale_y_continuous(limits = c(0,10))+ labs(title = &quot;OVEN - Katahdin Woods and Waters - 2023 sampling&quot;, x = &quot;Time (sampling window)&quot;, y = &quot;Counts&quot;, color = &quot;Counts&quot;, linetype = &quot;Counts&quot;, shape = &quot;Counts&quot;) + scale_linetype_manual(values = c(NA,&quot;dashed&quot;,&quot;solid&quot;,&quot;dashed&quot;))+ scale_shape_manual(values = c(21, NA,NA,NA)) + scale_color_manual(values = c(&quot;blue&quot;,&quot;darkgray&quot;,&quot;black&quot;,&quot;darkgray&quot;)) + theme_classic() + theme(legend.position = &quot;bottom&quot;) 8.3.2 Coding Acoustic detections counts StochGSS.acou.dc &lt;- function(){ # Priors on model parameters. Priors are DC1 in Lele et al (2007) a1 ~ dnorm(0,1); # constant, equivalent to the population growth rate. c1 ~ dunif(-1,1); # constant, the density dependence parameter. sig1 ~ dlnorm(-0.5,10); #variance parameter of stochastic environment (process noise) in the system stovar1 &lt;- 1/pow(sig1,2) # Priors for observation model alpha ~ dnorm(0, 0.001) # intercept for scaling gamma ~ dnorm(0, 0.001) # slope linking latent state X_t to mean r ~ dunif(0,50) # dispersion parameter for NegBin for(k in 1:K){ # Simulate trajectory that depends on the previous mean_X1[1,k] &lt;- a1/(1-c1) # Expected value of the first realization of the process # this is drawn from the stationary distribution of the process # Equation 14 (main text) and A.4 in Appendix of Dennis et al 2006 Varno1[k] &lt;- pow(sig1,2)/(1-pow(c1,2)) #. Equation A.5 in Appendix of Dennis et al 2006 # Updating the state: Stochastic process for all time steps X1[1,k]~dnorm(mean_X1[1,k], 1/Varno1[k]); #first estimation of population #iteration of the GSS model in the data for (t in 2:qp1) { mean_X1[t,k] &lt;- a1 + c1 * X1[(t - 1),k] X1[t,k] ~ dnorm(mean_X1[t,k], stovar1) # Et is included here since a+cX(t-1) + Et ~ Normal(a+cX(t-1),sigma^2) } # Updating the observations, from the acoustic detections under Negative Binomial observation error for (t in 1:qp1) { Y1[t,k] ~ dnegbin(p[t,k], r) p[t,k] &lt;- r/(r+lambda[t,k]) log(lambda[t,k]) &lt;- alpha + gamma * X1[t,k] } } } For this model, we will need some initial values of the parameters, and given that there could be many NA a wrap to provide those initial guess values. And we have to bundle the data for Data cloning. Let’s fit the first species in the taxa with higher data (Ovenbird in Marsh-Billings-Rockefeller NHP) as a test. ts.4guess &lt;- as.numeric(final_df$acous_detections) tvec4guess &lt;- 1:length(ts.4guess) onets4guess &lt;- cbind(tvec4guess, ts.4guess) naive.guess &lt;- guess.calc2.0(TimeAndNs = onets4guess) datalistGSS.acou.dc &lt;- list(K = 1, qp1 = length(ts.4guess), Y1 = dcdim(array(ts.4guess, dim = c(length(ts.4guess),1)))) dcrun.GSS.acou &lt;- dc.fit(data = datalistGSS.acou.dc, params = c(&quot;a1&quot;, &quot;c1&quot;, &quot;sig1&quot;, &quot;gamma&quot;, &quot;alpha&quot;, &quot;r&quot;), model = StochGSS.acou.dc, n.clones = c(1,5,10,20), multiply = &quot;K&quot;, unchanged = &quot;qp1&quot;, n.chains = 3, n.adapt = 5000, n.update = 100, thin = 20, n.iter = 20000) saveRDS(dcrun.GSS.acou, &quot;data/dcfitGSS_AcDet_2023_OVEN_KWW.rds&quot;) dcrun.GSS.acou &lt;- readRDS(&quot;data/dcfitGSS_AcDet_2023_OVEN_KWW.rds&quot;) summary(dcrun.GSS.acou); dcdiag(dcrun.GSS.acou) plot(dcdiag(dcrun.GSS.acou)) pairs(dcrun.GSS.acou) coef(dcrun.GSS.acou) Data cloning results for six parameters of GSS from acoustic detections Although diagnostics for lambda.max decreases with more clones, the chains are not converging. It seems that this needs more iterations and change initial values. Anyway, with these coefficients of the model, we can estimate latent count trajectories with the Kalman filter structure. This Kalman filter structure allows simultaneous estimation of latent states for observed time steps and prediction for missing ones, leveraging the temporal correlation in the process model. Kalman.pred.acou.fn &lt;- function() { # Priors on model parameters: they are on the real line. parms ~ dmnorm(MuPost,PrecPost) a1 &lt;- parms[1] alpha &lt;- parms[2] # intercept for scaling c1 &lt;- parms[3] gamma &lt;- parms[4] # slope linking latent state X_t to mean r &lt;- parms[5] # overdispersion parameter for NegBinom sig1 &lt;- parms[6] stovar1 &lt;- 1/pow(sig1,2) # Likelihood mean_X1[1] &lt;- a1/(1-c1) # Expected value of the first realization of the process # this is drawn from the stationary distribution of the process # Equation 14 (main text) and A.4 in Appendix of Dennis et al 2006 Varno1 &lt;- pow(sig1,2)/(1-pow(c1,2)) #. Equation A.5 in Appendix of Dennis et al 2006 # Updating the state: Stochastic process for all time steps X1[1]~dnorm(mean_X1[1], 1/Varno1); #first estimation of population log(lambda[1]) &lt;- alpha + gamma * X1[1] C[1] &lt;- log(lambda[1]) #iteration of the GSS model in the data for (t in 2:qp1) { mean_X1[t] &lt;- a1 + c1 * X1[(t - 1)] X1[t] ~ dnorm(mean_X1[t], stovar1) # Et is included here since a+cX(t-1) + Et ~ Normal(a+cX(t-1),sigma^2) Y1[(t-1)] ~ dnegbin(p[t], r) p[t] &lt;- r/(r+lambda[t]) log(lambda[t]) &lt;- alpha + gamma * X1[t] C[t] &lt;- log(lambda[t]) # expected count } } data4kalmanAco &lt;- list(qp1 = as.numeric(length(final_df$acous_detections)), Y1 = array(final_df$acous_detections, dim = c(as.numeric(length(final_df$acous_detections)))), MuPost = coef(dcrun.GSS.acou), PrecPost = solve(vcov(dcrun.GSS.acou))) And run the Bayesian inference using the MLE from Data cloning BH_DC_Pred_Acou = jags.fit(data=data4kalmanAco, params=c(&quot;C&quot;), model=Kalman.pred.acou.fn) And we can generate a dataframe of the time series with the estimates, inter quartile range, and observed data through time summary(BH_DC_Pred_Acou) # extract predictions and CI around them predAcou &lt;- as.data.frame(t(mcmcapply(BH_DC_Pred_Acou, quantile, c(0.025, 0.5, 0.975)))) ExpectedCountsAcou &lt;- as.data.frame(cbind(ExpectedCounts,predAcou)) # modify names names(ExpectedCountsAcou) &lt;- c(&quot;date&quot;,&quot;Time_w_alone&quot;,&quot;site_name&quot;,&quot;site_id&quot;, &quot;visit_number&quot;,&quot;common_name&quot;,&quot;time_window&quot;, &quot;Observed&quot;, &quot;AcousticDetections&quot;, &quot;DateTime&quot;,&quot;Time_window&quot;, &quot;year&quot;, &quot;time_id&quot;, &quot;CountLower&quot;, &quot;CountEstimated&quot;, &quot;CountUpper&quot;, &quot;AcouDetLower&quot;, &quot;AcouDetEstimated&quot;, &quot;AcouDetUpper&quot;) par(mar = c(5, 4, 7, 2)) # low, left, up, right plot(x = ExpectedCountsAcou$DateTime, y = ExpectedCountsAcou$Observed, col = &quot;#1b7837&quot;, ylim = c(0,40), ylab = &quot;Counts&quot;, xlab = &quot;Time (sampling window)&quot;) points(x = ExpectedCountsAcou$DateTime, y = ExpectedCountsAcou$AcousticDetections, col = &quot;#762a83&quot;) lines(x = ExpectedCountsAcou$DateTime, y = ExpectedCountsAcou$CountEstimated, col = &quot;#7fbf7b&quot;) lines(x = ExpectedCountsAcou$DateTime, y = ExpectedCountsAcou$CountLower, col = &quot;#d9f0d3&quot;, lty = 2) lines(x = ExpectedCountsAcou$DateTime, y = ExpectedCountsAcou$CountUpper, col = &quot;#d9f0d3&quot;, lty = 2) lines(x = ExpectedCountsAcou$DateTime, y = ExpectedCountsAcou$AcouDetEstimated, col = &quot;#af8dc3&quot;) lines(x = ExpectedCountsAcou$DateTime, y = ExpectedCountsAcou$AcouDetLower, col = &quot;#e7d4e8&quot;, lty = 2) lines(x = ExpectedCountsAcou$DateTime, y = ExpectedCountsAcou$AcouDetUpper, col = &quot;#e7d4e8&quot;, lty = 2) legend(x = min(ExpectedCountsAcou$DateTime), y = 65, cex = 0.7, pt.cex = 0.7, xpd = TRUE, horiz = FALSE, legend = c(&quot;Observed from point counts&quot;, &quot;Acoustic detections&quot;, &quot;Estimate from point counts&quot;, &quot;Point count CI estimates&quot;, &quot;Estimate from acoustic detections&quot;, &quot;Acoustic detections CI estimates&quot;), col = c(&quot;#1b7837&quot;, &quot;#762a83&quot;, &quot;#7fbf7b&quot;, &quot;#d9f0d3&quot;, &quot;#af8dc3&quot;, &quot;#e7d4e8&quot;), lty = c(NA, NA, 1, 2, 1, 2), # NA for points, 1 for solid lines, 2 for dashed pch = c(1, 1, NA, NA, NA, NA), # symbols for points bty = &quot;n&quot;) # no box around legend Observed and estimated counts from point counts and acoustic detections - A Cutting the edges to see the details of the estimates: par(mar = c(6, 4, 4, 2)) plot(x = ExpectedCountsAcou$DateTime, y = ExpectedCountsAcou$Observed, col = &quot;#1b7837&quot;, ylim = c(0,10), ylab = &quot;Counts&quot;, xlab = &quot;Time (sampling window)&quot;, main = &quot;OVEN - 2023 - Katahdin Woods and Waters&quot;) points(x = ExpectedCountsAcou$DateTime, y = ExpectedCountsAcou$AcousticDetections, col = &quot;#762a83&quot;) lines(x = ExpectedCountsAcou$DateTime, y = ExpectedCountsAcou$CountEstimated, col = &quot;#7fbf7b&quot;) lines(x = ExpectedCountsAcou$DateTime, y = ExpectedCountsAcou$CountLower, col = &quot;#d9f0d3&quot;, lty = 2) lines(x = ExpectedCountsAcou$DateTime, y = ExpectedCountsAcou$CountUpper, col = &quot;#d9f0d3&quot;, lty = 2) lines(x = ExpectedCountsAcou$DateTime, y = ExpectedCountsAcou$AcouDetEstimated, col = &quot;#af8dc3&quot;) lines(x = ExpectedCountsAcou$DateTime, y = ExpectedCountsAcou$AcouDetLower, col = &quot;#e7d4e8&quot;, lty = 2) lines(x = ExpectedCountsAcou$DateTime, y = ExpectedCountsAcou$AcouDetUpper, col = &quot;#e7d4e8&quot;, lty = 2) legend(x = quantile(ExpectedCountsAcou$DateTime, 0.75), y = 10, cex = 0.7, pt.cex = 0.7, xpd = TRUE, horiz = FALSE, legend = c(&quot;Observed from point counts&quot;, &quot;Acoustic detections&quot;, &quot;Estimate from point counts&quot;, &quot;Point count CI estimates&quot;, &quot;Estimate from acoustic detections&quot;, &quot;Acoustic detections CI estimates&quot;), col = c(&quot;#1b7837&quot;, &quot;#762a83&quot;, &quot;#7fbf7b&quot;, &quot;#d9f0d3&quot;, &quot;#af8dc3&quot;, &quot;#e7d4e8&quot;), lty = c(NA, NA, 1, 2, 1, 2), # NA for points, 1 for solid lines, 2 for dashed pch = c(1, 1, NA, NA, NA, NA), # symbols for points bty = &quot;n&quot;) # no box around legend Observed and estimated counts from point counts and acoustic detections - B "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
